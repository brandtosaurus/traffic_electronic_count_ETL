{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import rsa_headers as rh\n",
    "import rsa_data as rd\n",
    "import main\n",
    "import config\n",
    "import uuid\n",
    "from pandasql import sqldf\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "files = r\"C:\\Users\\MB27{}5851\\Desktop\\Temp\\rsa_traffic_counts\\SMEC RSA Files_GP PRM Sites_Dec21toFeb22\\{}337_2{}22{}131.RSA\"\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "testfile = r\"C:\\Users\\MB27{}5851\\Desktop\\Temp\\SYNTELL-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QRY = \"\"\"select * from trafc.electronic_count_data_type_10 t10 where exists (select header_id from trafc.electronic_count_header h where h.adt_total is null\n",
    "and h.header_id = t10.header_id)\"\"\"\n",
    "QRY_TEST =  \"\"\"select * from trafc.electronic_count_data_type_10 t10 where site_id = '0288' and year = 2021\"\"\"\n",
    "\n",
    "{HEADER_COLUMNS_QRY = \"\"\"select * from trafc.electronic_count_header limit 1\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_columns = list(pd.read_sql_query(HEADER_COLUMNS_QRY,config.ENGINE).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql_query(QRY_TEST, config.ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'site_id', 'header_id', 'year',\n",
       "       'number_of_fields_associated_with_the_basic_vehicle_data',\n",
       "       'data_source_code', 'edit_code', 'departure_date', 'departure_time',\n",
       "       'assigned_lane_number', 'physical_lane_number', 'forward_reverse_code',\n",
       "       'vehicle_category', 'vehicle_class_code_primary_scheme',\n",
       "       'vehicle_class_code_secondary_scheme', 'vehicle_speed',\n",
       "       'vehicle_length', 'site_occupancy_time_in_milliseconds',\n",
       "       'chassis_height_code', 'vehicle_following_code', 'vehicle_tag_code',\n",
       "       'trailer_count', 'axle_count', 'bumper_to_1st_axle_spacing',\n",
       "       'tyre_type', 'start_datetime', 'direction', 'data_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.74"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((data.loc[data['vehicle_speed']>speed_limit].count()[0]/data.count()[0])*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: empty expression not allowed (4249578818.py, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [98]\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: empty expression not allowed\n"
     ]
    }
   ],
   "source": [
    "def header_update(data):\n",
    "\tspeed_limit_qry = f\"select max_speed from trafc.countstation where tcname = '{data['site_id'][0]}' ;\"\n",
    "\tspeed_limit = pd.read_sql_query(speed_limit_qry,config.ENGINE)\n",
    "\tspeed_limit = speed_limit['max_speed'][0]\n",
    "\n",
    "\tUPDATE_STRING = f\"\"\"update\n",
    "\t\ttrafc.electronic_count_header\n",
    "\tset\n",
    "\t\ttotal_light_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='N')].count()[0].round().astype(int)},\n",
    "\t\ttotal_light_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='P')].count()[0].round().astype(int)},\n",
    "\t\ttotal_light_vehicles = {data.loc[data['vehicle_class_code_secondary_scheme']<=1].count()[0].round().astype(int)},\n",
    "\t\ttotal_heavy_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count()[0].round().astype(int)},\n",
    "\t\ttotal_heavy_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count()[0].round().astype(int)},\n",
    "\t\ttotal_heavy_vehicles = {data.loc[data['vehicle_class_code_secondary_scheme']>1].count()[0].round().astype(int)},\n",
    "\t\ttotal_short_heavy_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0].round().astype(int)},\n",
    "\t\ttotal_short_heavy_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0].round().astype(int)},\n",
    "\t\ttotal_short_heavy_vehicles = {data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0].round().astype(int)},\n",
    "\t\ttotal_medium_heavy_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0].round().astype(int)},\n",
    "\t\ttotal_medium_heavy_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0].round().astype(int)},\n",
    "\t\ttotal_medium_heavy_vehicles = {data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0].round().astype(int)},\n",
    "\t\ttotal_long_heavy_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0].round().astype(int)},\n",
    "\t\ttotal_long_heavy_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0].round().astype(int)},\n",
    "\t\ttotal_long_heavy_vehicles = {data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0].round().astype(int)},\n",
    "\t\ttotal_vehicles_positive_direction = {data.loc[data['direction']=='N'].count()[0].round().astype(int)},\n",
    "\t\ttotal_vehicles_negative_direction = {data.loc[data['direction']=='P'].count()[0].round().astype(int)},\n",
    "\t\ttotal_vehicles = {data.count()[0]},\n",
    "\t\taverage_speed_positive_direction = {data.loc[data['direction']=='N']['vehicle_speed'].mean().round(2)},\n",
    "\t\taverage_speed_negative_direction = {data.loc[data['direction']=='P']['vehicle_speed'].mean().round(2)},\n",
    "\t\taverage_speed = {data['vehicle_speed'].mean().round(2)},\n",
    "\t\taverage_speed_light_vehicles_positive_direction = {data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='N')].mean().round(2)},\n",
    "\t\taverage_speed_light_vehicles_negative_direction = {data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='P')].mean().round(2)},\n",
    "\t\taverage_speed_light_vehicles = {data['vehicle_speed'].loc[data['vehicle_class_code_secondary_scheme']<=1].mean().round(2)},\n",
    "\t\taverage_speed_heavy_vehicles_positive_direction = {data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].mean().round(2)},\n",
    "\t\taverage_speed_heavy_vehicles_negative_direction = {data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].mean().round(2)},\n",
    "\t\taverage_speed_heavy_vehicles = {data['vehicle_speed'].loc[data['vehicle_class_code_secondary_scheme']>1].mean().round(2)},\n",
    "\t\ttruck_split_positive_direction = '{str((((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count())[0])*100).round().astype(int))}',\n",
    "\t\ttruck_split_negative_direction = '{str((((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count())[0])*100).round().astype(int))}',\n",
    "\t\ttruck_split_total = '{str((((data.loc[data['vehicle_class_code_secondary_scheme']==2].count()/data.loc[data['vehicle_class_code_secondary_scheme']>1].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[data['vehicle_class_code_secondary_scheme']==3].count()/data.loc[data['vehicle_class_code_secondary_scheme']>1].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[data['vehicle_class_code_secondary_scheme']==4].count()/data.loc[data['vehicle_class_code_secondary_scheme']>1].count())[0])*100).round().astype(int))}',\n",
    "\t\testimated_axles_per_truck_negative_direction = {((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0]*2+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0]*5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0]*7)/(data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0])).round(2)},\n",
    "\t\testimated_axles_per_truck_positive_direction = {((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0]*2+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0]*5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0]*7)/(data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0])).round(2)},\n",
    "\t\testimated_axles_per_truck_total = {((data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0]*2+data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0]*5+data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0]*7)/(data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0]+data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0]+data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0])).round(2)},\n",
    "\t\tpercentage_speeding_positive_direction = {((data.loc[(data['vehicle_speed']>speed_limit)&(data['direction']=='P')].count()[0]/data.loc[data['direction'=='P']].count()[0])*100).round(2)},\n",
    "\t\tpercentage_speeding_negative_direction = {((data.loc[(data['vehicle_speed']>speed_limit)&(data['direction']=='N')].count()[0]/data.loc[data['direction'=='N']].count()[0])*100).round(2)},\n",
    "\t\tpercentage_speeding_total = {((data.loc[data['vehicle_speed']>speed_limit].count()[0]/data.count()[0])*100).round(2)},\n",
    "\t\tvehicles_with_rear_to_rear_headway_less_than_2sec_positive_dire = {data.loc[(data['vehicle_following_code']==2)&data['direction']=='N'].count()[0]},\n",
    "\t\tvehicles_with_rear_to_rear_headway_less_than_2sec_negative_dire = {data.loc[(data['vehicle_following_code']==2)&data['direction']=='P'].count()[0]},\n",
    "\t\tvehicles_with_rear_to_rear_headway_less_than_2sec_total = {data.loc[data['vehicle_following_code']==2].count()[0]},\n",
    "\t\testimated_e80_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0]*0.6+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0]*2.5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0]*2.1},\n",
    "\t\testimated_e80_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0]*0.6+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0]*2.5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0]*2.1},\n",
    "\t\testimated_e80_on_road = {data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0]*0.6+data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0]*2.5+data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0]*2.1},\n",
    "\t\tadt_positive_direction = {data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)},\n",
    "\t\tadt_negative_direction = {data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)},\n",
    "\t\tadt_total = {data.groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)},\n",
    "\t\tadtt_positive_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)},\n",
    "\t\tadtt_negative_direction = {data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)},\n",
    "\t\tadtt_total = {data.loc[data['vehicle_class_code_secondary_scheme']>1].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)},\n",
    "\t\thighest_volume_per_hour_positive_direction = {data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='H')).count().max()[0]},\n",
    "\t\thighest_volume_per_hour_negative_direction = {data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='H')).count().max()[0]},\n",
    "\t\thighest_volume_per_hour_total = {data.groupby(pd.Grouper(key='start_datetime',freq='H')).count().max()[0]},\n",
    "\t\t\"15th_highest_volume_per_hour_positive_direction\" = {data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.15)[0].astype(int)},\n",
    "\t\t\"15th_highest_volume_per_hour_negative_direction\" = {data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.15)[0].astype(int)},\n",
    "\t\t\"15th_highest_volume_per_hour_total\" = {data.groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.15)[0].astype(int)},\n",
    "\t\t\"30th_highest_volume_per_hour_positive_direction\" = {data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.30)[0].astype(int)},\n",
    "\t\t\"30th_highest_volume_per_hour_negative_direction\" = {data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.30)[0].astype(int)},\n",
    "\t\t\"30th_highest_volume_per_hour_total\" = {data.groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.30)[0].astype(int)},\n",
    "\t\t\"15th_percentile_speed_positive_direction\" = {data.loc[data['direction']=='N']['vehicle_speed'].quantile(0.15).round(2)},\n",
    "\t\t\"15th_percentile_speed_negative_direction\" = {data.loc[data['direction']=='P']['vehicle_speed'].quantile(0.15).round(2)},\n",
    "\t\t\"15th_percentile_speed_total\" = {data['vehicle_speed'].quantile(0.15).round(2)},\n",
    "\t\t\"85th_percentile_speed_positive_direction\" = {data.loc[data['direction']=='N']['vehicle_speed'].quantile(0.85).round(2)},\n",
    "\t\t\"85th_percentile_speed_negative_direction\" = {data.loc[data['direction']=='P']['vehicle_speed'].quantile(0.85).round(2)},\n",
    "\t\t\"85th_percentile_speed_total\" = {data['vehicle_speed'].quantile(0.85).round(2)},\n",
    "\t\tavg_weekday_traffic = {data.groupby(pd.Grouper(key='start_datetime',freq='B')).count().mean()[0].round().astype(int)},\n",
    "\t\tnumber_of_days_counted = {data.groupby([data['start_datetime'].dt.to_period('D')]).count().count()[0]},\n",
    "\t\tduration_hours = {data.groupby([data['start_datetime'].dt.to_period('H')]).count().count()[0]}\n",
    "\twhere\n",
    "\t\tsite_id = '{data['site_id'][0]}'\n",
    "\t\tand start_datetime = '{data['start_datetime'][0]}'\n",
    "\t\tand end_datetime = '{data['end_datetime'][0]}';\n",
    "\t\"\"\"\n",
    "\treturn UPDATE_STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_calcs(header, data, dtype):\n",
    "    data['start_datetime'] = pd.to_datetime(data['start_datetime'])\n",
    "    if dtype == 21:\n",
    "        header['adt_total'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['header_id']]).sum().mean()\n",
    "        header['adt_positive_direction'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "        header['adt_positive_direction'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "\n",
    "        header['adtt_total'] = data['total_heavy_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['header_id']]).sum().mean()\n",
    "        header['adtt_positive_direction'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "        header['adtt_positive_direction'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "\n",
    "        header['total_vehicles'] = data['total_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "        header['total_vehicles_positive_direction'] = data['total_vehicles_type21'].groupby(data['direction'].loc[data['direction'] == 'N']).sum()[{}]\n",
    "        header['total_vehicles_positive_direction'] = data['total_vehicles_type21'].groupby(data['direction'].loc[data['direction'] == 'N']).sum()[{}]\n",
    "\n",
    "        header['total_heavy_vehicles'] = data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "        header['total_heavy_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['total_heavy_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['truck_split_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}] / data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "        header['truck_split_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}] / data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "\n",
    "        header['total_light_vehicles'] = data['total_light_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "        header['total_light_positive_direction'] = data['total_light_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['total_light_positive_direction'] = data['total_light_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "\n",
    "        header['total_short_heavy_vehicles'] = data['short_heavy_vehicles'].groupby(data['header_id']).sum()[{}]\n",
    "        header['total_short_heavy_positive_direction'] = data['short_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['total_short_heavy_positive_direction'] = data['short_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "\n",
    "        header['total_medium_heavy_vehicles'] = data['medium_heavy_vehicles'].groupby(data['header_id']).sum()[{}]\n",
    "        header['total_medium_heavy_positive_direction'] = data['medium_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['total_medium_heavy_positive_direction'] = data['medium_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "\n",
    "        header['total_long_heavy_vehicles'] = data['long_heavy_vehicles'].groupby(data['header_id']).sum()[{}]\n",
    "        header['total_long_heavy_positive_direction'] = data['long_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['total_long_heavy_positive_direction'] = data['long_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "\n",
    "        header['vehicles_with_rear_to_rear_headway_less_than_2sec_positive_dire'] = data['rear_to_rear_headway_shorter_than_2_seconds'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['vehicles_with_rear_to_rear_headway_less_than_2sec_negative_dire'] = data['rear_to_rear_headway_shorter_than_2_seconds'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        header['vehicles_with_rear_to_rear_headway_less_than_2sec_total'] = data['rear_to_rear_headway_shorter_than_2_seconds'].groupby(data['header_id']).sum()[{}]\n",
    "    \n",
    "        header['type_21_count_interval_minutes'] = data['duration_min'].mean()\n",
    "\n",
    "        header['highest_volume_per_hour_positive_direction'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().max()\n",
    "        header['highest_volume_per_hour_negative_direction'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().max()\n",
    "        header['highest_volume_per_hour_total'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['header_id']]).sum().max()\n",
    "\n",
    "        header['15th_highest_volume_per_hour_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().quantile(q={}.15,  interpolation='linear'))\n",
    "        header['15th_highest_volume_per_hour_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().quantile(q={}.15,  interpolation='linear'))\n",
    "        header['15th_highest_volume_per_hour_total'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['header_id']]).sum().quantile(q={}.15, interpolation='linear'))\n",
    "        \n",
    "        header['3{}th_highest_volume_per_hour_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().quantile(q={}.3,  interpolation='linear'))\n",
    "        header['3{}th_highest_volume_per_hour_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().quantile(q={}.3, interpolation='linear'))\n",
    "        header['3{}th_highest_volume_per_hour_total'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['header_id']]).sum().quantile(q={}.3, interpolation='linear'))\n",
    "\n",
    "        # header['average_speed_positive_direction'] = \n",
    "        # header['average_speed_negative_direction'] = \n",
    "        header['average_speed'] = ((\n",
    "            (header['speedbin1'] * data['speedbin1'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin2'] * data['speedbin2'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin3'] * data['speedbin3'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin4'] * data['speedbin4'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin5'] * data['speedbin5'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin6'] * data['speedbin6'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin7'] * data['speedbin7'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin8'] * data['speedbin8'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin9'] * data['speedbin9'].groupby(data['header_id']).sum()[{}]) \n",
    "            )\n",
    "            / data['sum_of_heavy_vehicle_speeds'].groupby(data['header_id']).sum()[{}]\n",
    "            )\n",
    "        # header['average_speed_light_vehicles_positive_direction'] = \n",
    "        # header['average_speed_light_vehicles_negative_direction'] = \n",
    "        header['average_speed_light_vehicles'] = ((\n",
    "            (header['speedbin1'] * data['speedbin1'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin2'] * data['speedbin2'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin3'] * data['speedbin3'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin4'] * data['speedbin4'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin5'] * data['speedbin5'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin6'] * data['speedbin6'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin7'] * data['speedbin7'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin8'] * data['speedbin8'].groupby(data['header_id']).sum()[{}]) +\n",
    "            (header['speedbin9'] * data['speedbin9'].groupby(data['header_id']).sum()[{}]) -\n",
    "            data['sum_of_heavy_vehicle_speeds'].groupby(data['header_id']).sum()[{}]\n",
    "            )\n",
    "            / data['sum_of_heavy_vehicle_speeds'].groupby(data['header_id']).sum()[{}]\n",
    "            )\n",
    "        \n",
    "        # header['average_speed_heavy_vehicles_positive_direction'] = \n",
    "        # header['average_speed_heavy_vehicles_negative_direction'] = \n",
    "        # header['average_speed_heavy_vehicles'] = \n",
    "\n",
    "        header['truck_split_positive_direction'] = (str(round(data['short_heavy_vehicles'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}]*1{}{})) + ' : ' +\n",
    "        str(round(data['medium_heavy_vehicles'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}]*1{}{})) + ' : ' +\n",
    "        str(round(data['long_heavy_vehicles'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}]*1{}{}))\n",
    "        )\n",
    "        header['truck_split_negative_direction'] = (str(round(data['short_heavy_vehicles'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}]*1{}{})) + ' : ' +\n",
    "        str(round(data['medium_heavy_vehicles'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}]*1{}{})) + ' : ' +\n",
    "        str(round(data['long_heavy_vehicles'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby([data['header_id'], data['direction'].loc[data['direction'] == 'N']]).sum()[{}]*1{}{}))\n",
    "        )\n",
    "        header['truck_split_total'] = (str(round(data['short_heavy_vehicles'].groupby(data['header_id']).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]*1{}{})) + ' : ' +\n",
    "        str(round(data['medium_heavy_vehicles'].groupby(data['header_id']).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]*1{}{})) + ' : ' +\n",
    "        str(round(data['long_heavy_vehicles'].groupby(data['header_id']).sum()[{}] / \n",
    "        data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]*1{}{}))\n",
    "        )\n",
    "\n",
    "        return header\n",
    "    elif dtype == 3{}:\n",
    "        if header['adt_total'].isnull().all():\n",
    "            header['adt_total'] = data['total_vehicles_type3{}'].groupby([data['start_datetime'].dt.to_period('D'), data['header_id']]).sum().mean()\n",
    "            header['adt_positive_direction'] = data['total_vehicles_type3{}'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "            header['adt_positive_direction'] = data['total_vehicles_type3{}'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if header['adtt_total'].isnull().all():\n",
    "            header['adtt_total'] = data['total_heavy_vehicles_type3{}'].groupby([data['start_datetime'].dt.to_period('D'), data['header_id']]).sum().mean()\n",
    "            header['adtt_positive_direction'] = data['total_vehicles_type3{}'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "            header['adtt_positive_direction'] = data['total_vehicles_type3{}'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum().mean()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if header['total_vehicles'].isnull().all():\n",
    "            header['total_vehicles'] = data['total_vehicles_type3{}'].groupby(data['header_id']).sum()[{}]\n",
    "            header['total_vehicles_positive_direction'] = data['total_vehicles_type3{}'].groupby(data['direction'].loc[data['direction'] == 'N']).sum()[{}]\n",
    "            header['total_vehicles_positive_direction'] = data['total_vehicles_type3{}'].groupby(data['direction'].loc[data['direction'] == 'N']).sum()[{}]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if header['total_heavy_vehicles'].isnull().all():\n",
    "            header['total_heavy_vehicles'] = data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "            header['total_heavy_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "            header['total_heavy_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "            header['truck_split_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}] / data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "            header['truck_split_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}] / data['total_heavy_vehicles_type21'].groupby(data['header_id']).sum()[{}]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if header['total_light_vehicles'].isnull().all():\n",
    "            header['total_light_vehicles'] = data['total_light_vehicles_type3{}'].groupby(data['header_id']).sum()[{}]\n",
    "            header['total_light_positive_direction'] = data['total_light_vehicles_type3{}'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "            header['total_light_positive_direction'] = data['total_light_vehicles_type3{}'].groupby([data['direction'].loc[data['direction'] == 'N'], data['header_id']]).sum()[{}]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return header\n",
    "\n",
    "    elif dtype == 7{}:\n",
    "    \n",
    "        return header\n",
    "\n",
    "    elif dtype == 1{}:\n",
    "    \n",
    "        return header\n",
    "\n",
    "    elif dtype == 6{}:\n",
    "        \n",
    "        return header\n",
    "\n",
    "    else:\n",
    "        return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_out = header_calcs(header, data, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_out = header_calcs(header_out, data, 3{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek({})\n",
    "\n",
    "        columns = \", \".join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = \"{}.{}\".format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = \"COPY {} ({}) FROM STDIN WITH CSV\".format(table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_partitioned_table(df, table, subset) -> None:\n",
    "    yr = data['year'].unique()[{}]\n",
    "    print(yr)\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            table+'_'+str(yr),\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "    except Exception:\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        df.to_sql(\n",
    "            table+'_'+yr,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "\n",
    "def push_to_db(df, table, subset) -> None:\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "    except Exception:\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_upsert(table, conn, keys, data_iter):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "\n",
    "    insert_statement = insert(table.table).values(data)\n",
    "    upsert_statement = insert_statement.on_conflict_do_update(\n",
    "        constraint=f\"{table.table.name}_un\",\n",
    "        set_={c.key: c for c in insert_statement.excluded},\n",
    "    )\n",
    "    conn.execute(upsert_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_upsert_1(table, conn, keys, data_iter):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "\n",
    "    insert_statement = insert(table.table).values(data)\n",
    "    upsert_statement = insert_statement.on_conflict_do_update(\n",
    "        constraint=f\"{table.table.name}_un\",\n",
    "        set_={c.key: c for c in insert_statement.excluded},\n",
    "    )\n",
    "    conn.execute(upsert_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_statement = insert(table.table).values(data)\n",
    "upsert_statement = insert_statement.on_conflict_do_update(\n",
    "    constraint=f\"{table.table.name}_un\",\n",
    "    set_={c.key: c for c in insert_statement.excluded else },\n",
    ")\n",
    "conn.execute(upsert_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\test.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000018?line=0'>1</a>\u001b[0m push_to_partitioned_table(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000018?line=1'>2</a>\u001b[0m             data,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000018?line=2'>3</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39melectronic_count_data_partitioned\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000018?line=3'>4</a>\u001b[0m             [\u001b[39m\"\u001b[39;49m\u001b[39msite_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstart_datetime\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlane_number\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000018?line=4'>5</a>\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\test.ipynb Cell 15'\u001b[0m in \u001b[0;36mpush_to_partitioned_table\u001b[1;34m(df, table, subset)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpush_to_partitioned_table\u001b[39m(df, table, subset) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=1'>2</a>\u001b[0m     yr \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=2'>3</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=3'>4</a>\u001b[0m         df\u001b[39m.\u001b[39mto_sql(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=4'>5</a>\u001b[0m             table\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(yr),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=5'>6</a>\u001b[0m             con\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mENGINE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=9'>10</a>\u001b[0m             method\u001b[39m=\u001b[39mpsql_insert_copy,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000017?line=10'>11</a>\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=954'>955</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=956'>957</a>\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=957'>958</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=959'>960</a>\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=960'>961</a>\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=961'>962</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=962'>963</a>\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=1065'>1066</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=1067'>1068</a>\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=1068'>1069</a>\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=1069'>1070</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/MB2705851/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "push_to_partitioned_table(\n",
    "            data,\n",
    "            \"electronic_count_data_partitioned\",\n",
    "            [\"site_id\", \"start_datetime\", \"lane_number\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_to_db(\n",
    "    header_out,\n",
    "    \"electronic_count_header\",\n",
    "    [\"site_id\", \"start_datetime\", \"end_datetime\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].unique()[{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\Users\\MB27{}5851\\Desktop\\Temp\\rsa_traffic_counts\\1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d86798f8e6233a76aa38644d13f9f426c72f43ddd8d5316bb9701fa2c43845d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
