{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rsa_data as rd\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tools\n",
    "from io import StringIO\n",
    "import csv\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "src = r\"S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\"\n",
    "test1 = r\"S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\0006-20211231.RSV\"\n",
    "problem_files = [\n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0263-20220228.RSV', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\Manuals & Guidelines\\Traffic\\example data\\1114-20200229-240000.RSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfiles(path: str):\n",
    "    print(\"COLLECTING FILES......\")\n",
    "    src = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if (\n",
    "                name.endswith(\".RSA\")\n",
    "                or name.endswith(\".rsa\")\n",
    "                or name.endswith(\".rsv\")\n",
    "                or name.endswith(\".RSV\")\n",
    "            ):\n",
    "                p = os.path.join(root, name)\n",
    "                src.append(p)\n",
    "    src = list(set(src))\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file, header=None, sep=\" \", low_memory=False)\n",
    "    df = df[0].str.split(\"\\s+|,\\s+|,\", expand=True)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "def push_to_db(df, table, subset) -> None:\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "    except Exception:\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = \", \".join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = \"{}.{}\".format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = \"COPY {} ({}) FROM STDIN WITH CSV\".format(table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "def join(header: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    if data.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        q = \"\"\"\n",
    "\t\tSELECT header.header_id, header.station_name, data.*\n",
    "\t\tFROM header\n",
    "\t\tLEFT JOIN data ON data.start_datetime WHERE data.start_datetime >= header.start_datetime AND data.end_datetime <= header.end_datetime;\n",
    "\t\t\"\"\"\n",
    "        q2 = \"\"\"UPDATE data set header_id = (SELECT header_id from header WHERE data.start_datetime >= header.start_datetime AND data.counttime_end <= header.enddate)\"\"\"\n",
    "        pysqldf = lambda q: sqldf(q, globals())\n",
    "        df = sqldf(q, locals())\n",
    "        df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_join(data: pd.DataFrame, header: pd.DataFrame) -> pd.DataFrame:\n",
    "    if data is None:\n",
    "        pass\n",
    "    elif data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        data = pd.DataFrame(data)\n",
    "        data = join(header, data)\n",
    "    return data\n",
    "\n",
    "def get_direction(lane_number, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        filt = df[1] == lane_number\n",
    "        df = df.where(filt)\n",
    "        df = df[2].dropna()\n",
    "        df = int(df)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head(df) -> pd.DataFrame:\n",
    "    dfh = pd.DataFrame(\n",
    "        df.loc[\n",
    "            (df[0].isin([\"H0\", \"S0\", \"I0\", \"S1\", \"D0\", \"D1\", \"D3\", \"L0\", \"L1\"]))\n",
    "            | (\n",
    "                (df[0].isin([\"21\", \"70\", \"30\", \"13\", \"60\"]))\n",
    "                & (~df[1].isin([\"0\", \"1\", \"2\", \"3\", \"4\"]))\n",
    "            )\n",
    "            | (\n",
    "                (df[0].isin([\"10\"]))\n",
    "                & (df[1].isin([\"1\", \"8\", \"5\", \"01\", \"08\", \"05\"]))\n",
    "            )\n",
    "        ]\n",
    "    ).dropna(axis=1, how=\"all\")\n",
    "    dfh[\"index\"] = dfh.index\n",
    "    breaks = dfh[\"index\"].diff() != 1\n",
    "    groups = breaks.cumsum()\n",
    "    dfh[\"newindex\"] = groups\n",
    "    dfh = dfh.set_index(\"newindex\")\n",
    "    dfh = dfh.drop(columns=[\"index\"])\n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers(dfh: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not dfh.empty:\n",
    "        headers = pd.DataFrame()\n",
    "        headers[\"site_id\"] = dfh.loc[dfh[0] == \"S0\", 1].astype(str)\n",
    "        if not dfh.loc[dfh[0] == \"S1\", 1:].empty:\n",
    "            headers[\"station_name\"] = (\n",
    "                dfh.loc[dfh[0] == \"S1\", 1:]\n",
    "                .dropna(axis=1)\n",
    "                .apply(\" \".join, axis=1)\n",
    "                .astype(str)\n",
    "            )\n",
    "        else:\n",
    "            headers[\"station_name\"] = (\n",
    "                dfh.loc[dfh[0] == \"S0\", 2:]\n",
    "                .dropna(axis=1)\n",
    "                .apply(\" \".join, axis=1)\n",
    "                .astype(str)\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            headers[\"y\"] = dfh.loc[dfh[0] == \"S0\", 5].astype(float)\n",
    "            headers[\"x\"] = dfh.loc[dfh[0] == \"S0\", 6].astype(float)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        headers[\"number_of_lanes\"] = dfh.loc[dfh[0] == \"L0\", 2].astype(int)\n",
    "        headers[\"primary_direction\"] = int(dfh.loc[dfh[0] == 'L1', 2].unique()[0])\n",
    "        try:\n",
    "            headers[\"secondary_direction\"] = dfh.loc[dfh[0] == 'L'+ headers[\"number_of_lanes\"].iloc[0].astype(str), 2].astype(int) \n",
    "        except:\n",
    "            headers[\"secondary_direction\"] = int(dfh.loc[dfh[0] == 'L1', 2].unique()[1])\n",
    "            \n",
    "\n",
    "        try:\n",
    "            headers[\"speedbin1\"] = dfh.loc[dfh[0] == \"21\", 4].astype(int)\n",
    "            headers[\"speedbin2\"] = dfh.loc[dfh[0] == \"21\", 5].astype(int)\n",
    "            headers[\"speedbin3\"] = dfh.loc[dfh[0] == \"21\", 6].astype(int)\n",
    "            headers[\"speedbin4\"] = dfh.loc[dfh[0] == \"21\", 7].astype(int)\n",
    "            headers[\"speedbin5\"] = dfh.loc[dfh[0] == \"21\", 8].astype(int)\n",
    "            headers[\"speedbin6\"] = dfh.loc[dfh[0] == \"21\", 9].astype(int)\n",
    "            headers[\"speedbin7\"] = dfh.loc[dfh[0] == \"21\", 10].astype(int)\n",
    "            headers[\"speedbin8\"] = dfh.loc[dfh[0] == \"21\", 11].astype(int)\n",
    "            headers[\"speedbin9\"] = dfh.loc[dfh[0] == \"21\", 12].astype(int)\n",
    "            headers[\"type_21_count_interval_minutes\"] = dfh.loc[\n",
    "                dfh[0] == \"21\", 1\n",
    "            ].astype(int)\n",
    "            headers[\"type_21_programmable_rear_to_rear_headway_bin\"] = dfh.loc[\n",
    "                dfh[0] == \"21\", 3\n",
    "            ].astype(int)\n",
    "            headers[\"type_21_program_id\"] = \"2\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_10_vehicle_classification_scheme_primary\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 1\n",
    "            ].astype(int)\n",
    "            headers[\"type_10_vehicle_classification_scheme_secondary\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 2\n",
    "            ].astype(int)\n",
    "            headers[\"type_10_maximum_gap_milliseconds\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 3\n",
    "            ].astype(int)\n",
    "            headers[\"type_10_maximum_differential_speed\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 4\n",
    "            ].astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_30_summary_interval_minutes\"] = dfh.loc[\n",
    "                dfh[0] == \"30\", 2\n",
    "            ].astype(int)\n",
    "            headers[\"type_30_vehicle_classification_scheme\"] = dfh.loc[\n",
    "                dfh[0] == \"30\", 3\n",
    "            ].astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_70_summary_interval_minutes\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 1\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_vehicle_classification_scheme\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 2\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_maximum_gap_milliseconds\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 3\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_maximum_differential_speed\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 4\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_error_bin_code\"] = dfh.loc[dfh[0] == \"70\", 5].astype(\n",
    "                int\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not dfh.loc[dfh[0] == \"D3\", 1].empty:\n",
    "            headers[\"start_datetime\"] = dfh.loc[dfh[0] == \"D3\", 1].astype(str)\n",
    "            headers[\"start_time\"] = dfh.loc[dfh[0] == \"D3\", 2].astype(str)\n",
    "            headers[\"end_datetime\"] = dfh.loc[dfh[0] == \"D3\", 3].astype(str)\n",
    "            headers[\"end_time\"] = dfh.loc[dfh[0] == \"D3\", 4].astype(str)\n",
    "        else:\n",
    "            headers[\"start_datetime\"] = dfh.loc[dfh[0] == \"D1\", 1].astype(str)\n",
    "            headers[\"start_time\"] = dfh.loc[dfh[0] == \"D1\", 2].astype(str)\n",
    "            headers[\"end_datetime\"] = dfh.loc[dfh[0] == \"D1\", 3].astype(str)\n",
    "            headers[\"end_time\"] = dfh.loc[dfh[0] == \"D1\", 4].astype(str)\n",
    "\n",
    "        try:\n",
    "\n",
    "            for idx, row in headers.iterrows():\n",
    "                if len(row['start_datetime']) == 6 and len(row['start_time']) == 6:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['start_datetime'] + row['start_time'], format='%y%m%d%H%M%S')\n",
    "                elif len(row['start_datetime']) == 8 and len(row['start_time']) == 6:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['start_datetime'] + row['start_time'], format='%Y%m%d%H%M%S')\n",
    "                elif len(row['start_datetime']) == 6 and len(row['start_time']) == 8:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['start_datetime'] + row['start_time'], format='%y%m%d%H%M%S%f')\n",
    "                elif len(row['start_datetime']) == 8 and len(row['start_time']) == 8:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['start_datetime'] + row['start_time'], format='%Y%m%d%H%M%S%f')\n",
    "\n",
    "                if len(row['end_datetime']) == 6 and row['end_time'].strip('0') == '24':\n",
    "                    row['end_datetime'] = pd.to_datetime(row['end_datetime'], format='%y%m%d')+timedelta(days=1)\n",
    "                elif len(row['end_datetime']) ==8 and row['end_time'].strip('0') == '24':\n",
    "                    row['end_datetime'] = pd.to_datetime(row['end_datetime'], format='%Y%m%d')+timedelta(days=1)\n",
    "                elif len(row['end_datetime']) == 6 and len(row['end_time']) == 6:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['end_datetime'] + row['end_time'], format='%y%m%d%H%M%S')\n",
    "                elif len(row['end_datetime']) == 8 and len(row['end_time']) == 6:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['end_datetime'] + row['end_time'], format='%Y%m%d%H%M%S')\n",
    "                elif len(row['end_datetime']) == 6 and len(row['end_time']) == 8:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['end_datetime'] + row['end_time'], format='%y%m%d%H%M%S%f')\n",
    "                elif len(row['end_datetime']) == 8 and len(row['end_time']) == 8:\n",
    "                    row['start_datetime'] = pd.to_datetime(row['end_datetime'] + row['end_time'], format='%Y%m%d%H%M%S%f')\n",
    "\n",
    "        except:\n",
    "        \n",
    "            headers[\"end_datetime\"] = headers.apply(\n",
    "                lambda x: pd.to_datetime(\n",
    "                    x[\"end_datetime\"] + x[\"end_time\"], format=\"%y%m%d%H%M%S\"\n",
    "                )\n",
    "                if (\n",
    "                    x[\"end_time\"] != \"240000\"\n",
    "                    and len(x[\"end_datetime\"]) == 6\n",
    "                    and len(x[\"end_time\"]) == 6\n",
    "                )\n",
    "                else (\n",
    "                    pd.to_datetime(\n",
    "                        x[\"end_datetime\"] + x[\"end_time\"], format=\"%y%m%d%H%M%S%f\"\n",
    "                    )\n",
    "                    if (\n",
    "                        x[\"end_time\"] != \"24000000\"\n",
    "                        and len(x[\"end_datetime\"]) == 6\n",
    "                        and len(x[\"end_time\"]) == 8\n",
    "                    )\n",
    "                    else (\n",
    "                        pd.to_datetime(\n",
    "                            x[\"end_datetime\"] + x[\"end_time\"], format=\"%Y%m%d%H%M%S\"\n",
    "                        )\n",
    "                        if (\n",
    "                            x[\"end_time\"] != \"240000\"\n",
    "                            and len(x[\"end_datetime\"]) == 8\n",
    "                            and len(x[\"end_time\"]) == 6\n",
    "                        )\n",
    "                        else (\n",
    "                            pd.to_datetime(\n",
    "                                x[\"end_datetime\"] + x[\"end_time\"],\n",
    "                                format=\"%Y%m%d%H%M%S%f\",\n",
    "                            )\n",
    "                            if (\n",
    "                                x[\"end_time\"] != \"24000000\"\n",
    "                                and len(x[\"end_datetime\"]) == 8\n",
    "                                and len(x[\"end_time\"]) == 8\n",
    "                            )\n",
    "                            else (\n",
    "                                pd.to_datetime(x[\"end_datetime\"], format=\"%y%m%d\")\n",
    "                                + timedelta(days=1)\n",
    "                                if (\n",
    "                                    x[\"end_time\"] == \"240000\"\n",
    "                                    and len(x[\"end_datetime\"]) == 6\n",
    "                                    and len(x[\"end_time\"]) == 6\n",
    "                                )\n",
    "                                else (\n",
    "                                    pd.to_datetime(x[\"end_datetime\"], format=\"%y%m%d\")\n",
    "                                    + timedelta(days=1)\n",
    "                                    if (\n",
    "                                        x[\"end_time\"] == \"24000000\"\n",
    "                                        and len(x[\"end_datetime\"]) == 6\n",
    "                                        and len(x[\"end_time\"]) == 8\n",
    "                                    )\n",
    "                                    else (\n",
    "                                        pd.to_datetime(\n",
    "                                            x[\"end_datetime\"], format=\"%Y%m%d\"\n",
    "                                        )\n",
    "                                        + timedelta(days=1)\n",
    "                                        if (\n",
    "                                            x[\"end_time\"] == \"240000\"\n",
    "                                            and len(x[\"end_datetime\"]) == 8\n",
    "                                            and len(x[\"end_time\"]) == 6\n",
    "                                        )\n",
    "                                        else (\n",
    "                                            pd.to_datetime(\n",
    "                                                x[\"end_datetime\"], format=\"%Y%m%d\"\n",
    "                                            )\n",
    "                                            + timedelta(days=1)\n",
    "                                            if (\n",
    "                                                x[\"end_time\"] == \"24000000\"\n",
    "                                                and len(x[\"end_datetime\"]) == 8\n",
    "                                                and len(x[\"end_time\"]) == 8\n",
    "                                            )\n",
    "                                            else (pd.to_datetime(\n",
    "                                            x[\"end_datetime\"] + x[\"end_time\"], format=\"%y%m%d%H%M%S%f\"\n",
    "                                            if (len(x[\"end_datetime\"]) == 6 and len(x[\"end_time\"]) == 7)\n",
    "                                                else pd.to_datetime(\n",
    "                                                    x[\"end_datetime\"] + x[\"end_time\"]\n",
    "                                                ))\n",
    "                                            )\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            headers[\"start_datetime\"] = headers.apply(\n",
    "            lambda x: pd.to_datetime(\n",
    "                x[\"start_datetime\"] + x[\"start_time\"], format=\"%y%m%d%H%M%S\"\n",
    "            )\n",
    "            if (\n",
    "                x[\"start_time\"] != \"240000\"\n",
    "                and len(x[\"start_datetime\"]) == 6\n",
    "                and len(x[\"start_time\"]) == 6\n",
    "            )\n",
    "            else (\n",
    "                pd.to_datetime(\n",
    "                    x[\"start_datetime\"] + x[\"start_time\"], format=\"%y%m%d%H%M%S%f\"\n",
    "                )\n",
    "                if (\n",
    "                    x[\"start_time\"] != \"24000000\"\n",
    "                    and len(x[\"start_datetime\"]) == 6\n",
    "                    and len(x[\"start_time\"]) == 8\n",
    "                )\n",
    "                else (\n",
    "                    pd.to_datetime(\n",
    "                        x[\"start_datetime\"] + x[\"start_time\"], format=\"%Y%m%d%H%M%S\"\n",
    "                    )\n",
    "                    if (\n",
    "                        x[\"start_time\"] != \"240000\"\n",
    "                        and len(x[\"start_datetime\"]) == 8\n",
    "                        and len(x[\"start_time\"]) == 6\n",
    "                    )\n",
    "                    else (\n",
    "                        pd.to_datetime(\n",
    "                            x[\"start_datetime\"] + x[\"start_time\"],\n",
    "                            format=\"%Y%m%d%H%M%S%f\",\n",
    "                        )\n",
    "                        if (\n",
    "                            x[\"start_time\"] != \"24000000\"\n",
    "                            and len(x[\"start_datetime\"]) == 8\n",
    "                            and len(x[\"start_time\"]) == 8\n",
    "                        )\n",
    "                        else (\n",
    "                            pd.to_datetime(x[\"start_datetime\"], format=\"%y%m%d\")\n",
    "                            + timedelta(days=1)\n",
    "                            if (\n",
    "                                x[\"start_time\"] == \"240000\"\n",
    "                                and len(x[\"start_datetime\"]) == 6\n",
    "                                and len(x[\"start_time\"]) == 6\n",
    "                            )\n",
    "                            else (\n",
    "                                pd.to_datetime(x[\"start_datetime\"], format=\"%y%m%d\")\n",
    "                                + timedelta(days=1)\n",
    "                                if (\n",
    "                                    x[\"start_time\"] == \"24000000\"\n",
    "                                    and len(x[\"start_datetime\"]) == 6\n",
    "                                    and len(x[\"start_time\"]) == 8\n",
    "                                )\n",
    "                                else (\n",
    "                                    pd.to_datetime(\n",
    "                                        x[\"start_datetime\"], format=\"%Y%m%d\"\n",
    "                                    )\n",
    "                                    + timedelta(days=1)\n",
    "                                    if (\n",
    "                                        x[\"start_time\"] == \"240000\"\n",
    "                                        and len(x[\"start_datetime\"]) == 8\n",
    "                                        and len(x[\"start_time\"]) == 6\n",
    "                                    )\n",
    "                                    else (\n",
    "                                        pd.to_datetime(\n",
    "                                            x[\"start_datetime\"], format=\"%Y%m%d\"\n",
    "                                        )\n",
    "                                        + timedelta(days=1)\n",
    "                                        if (\n",
    "                                            x[\"start_time\"] == \"24000000\"\n",
    "                                            and len(x[\"start_datetime\"]) == 8\n",
    "                                            and len(x[\"start_time\"]) == 8\n",
    "                                        )\n",
    "                                        else (\n",
    "                                        pd.to_datetime(\n",
    "                                            x[\"start_datetime\"] + x[\"start_time\"], format=\"%y%m%d%H%M%S%f\"\n",
    "                                        )\n",
    "                                        if (\n",
    "                                            len(x[\"start_datetime\"]) == 6\n",
    "                                            and len(x[\"start_time\"]) == 7\n",
    "                                        )\n",
    "                                            else pd.to_datetime(\n",
    "                                                x[\"start_datetime\"] + x[\"start_time\"]\n",
    "                                            )\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        headers = headers.drop([\"start_time\"], axis=1)\n",
    "        headers = headers.drop([\"end_time\"], axis=1)\n",
    "\n",
    "        headers[\"start_datetime\"] = pd.to_datetime(headers[\"start_datetime\"])\n",
    "        headers[\"end_datetime\"] = pd.to_datetime(headers[\"end_datetime\"])\n",
    "        headers[\"site_id\"] = headers[\"site_id\"].astype(str)\n",
    "\n",
    "        try:\n",
    "            headers[\"instrumentation_description\"] = (\n",
    "                dfh.loc[dfh[0] == \"I0\", 1:]\n",
    "                .dropna(axis=1)\n",
    "                .apply(\" \".join, axis=1)\n",
    "                .astype(str)\n",
    "            )\n",
    "        except Exception:\n",
    "            headers[\"instrumentation_description\"] = None\n",
    "\n",
    "        try:\n",
    "            headers[\"type_30_summary_interval_minutes\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_70_summary_interval_minutes\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "            headers[\"type_70_vehicle_classification_scheme\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "            headers[\"type_70_vehicle_classification_scheme\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        headers = headers.fillna(method=\"ffill\")\n",
    "        headers = headers.fillna(method=\"bfill\")\n",
    "\n",
    "        headers = headers.drop_duplicates(ignore_index=True)\n",
    "\n",
    "        headers[\"header_id\"] = \"\"\n",
    "        headers[\"header_id\"] = headers[\"header_id\"].apply(\n",
    "            lambda x: str(uuid.uuid4())\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=to_df(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = get_head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = tools.headers(dfh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>number_of_lanes</th>\n",
       "      <th>primary_direction</th>\n",
       "      <th>secondary_direction</th>\n",
       "      <th>speedbin1</th>\n",
       "      <th>speedbin2</th>\n",
       "      <th>speedbin3</th>\n",
       "      <th>speedbin4</th>\n",
       "      <th>speedbin5</th>\n",
       "      <th>speedbin6</th>\n",
       "      <th>speedbin7</th>\n",
       "      <th>speedbin8</th>\n",
       "      <th>speedbin9</th>\n",
       "      <th>type_21_count_interval_minutes</th>\n",
       "      <th>type_21_programmable_rear_to_rear_headway_bin</th>\n",
       "      <th>type_21_program_id</th>\n",
       "      <th>type_10_vehicle_classification_scheme_primary</th>\n",
       "      <th>type_10_vehicle_classification_scheme_secondary</th>\n",
       "      <th>type_10_maximum_gap_milliseconds</th>\n",
       "      <th>type_10_maximum_differential_speed</th>\n",
       "      <th>type_30_summary_interval_minutes</th>\n",
       "      <th>type_70_summary_interval_minutes</th>\n",
       "      <th>type_70_vehicle_classification_scheme</th>\n",
       "      <th>type_70_maximum_gap_milliseconds</th>\n",
       "      <th>type_70_maximum_differential_speed</th>\n",
       "      <th>type_70_error_bin_code</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>instrumentation_description</th>\n",
       "      <th>header_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114</td>\n",
       "      <td>1114 Huguenot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>140</td>\n",
       "      <td>15</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3000</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 00:00:00</td>\n",
       "      <td>2020-02-06 12:36:23.700</td>\n",
       "      <td>00001 RT8000</td>\n",
       "      <td>40a2f39a-2b0d-48b0-ba99-8dee58fba495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1114</td>\n",
       "      <td>1114 Huguenot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>140</td>\n",
       "      <td>15</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3000</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-06 12:37:47</td>\n",
       "      <td>2020-02-29 02:40:00.000</td>\n",
       "      <td>00001 RT8000</td>\n",
       "      <td>db7020ae-e86f-420d-8a4d-c10fb41dc91c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_id   station_name   y   x  number_of_lanes  primary_direction  \\\n",
       "0    1114  1114 Huguenot NaN NaN                2                  0   \n",
       "1    1114  1114 Huguenot NaN NaN                2                  0   \n",
       "\n",
       "   secondary_direction  speedbin1  speedbin2  speedbin3  speedbin4  speedbin5  \\\n",
       "0                  NaN         60         70         80         90        100   \n",
       "1                  NaN         60         70         80         90        100   \n",
       "\n",
       "   speedbin6  speedbin7  speedbin8  speedbin9  type_21_count_interval_minutes  \\\n",
       "0        110        120        130        140                              15   \n",
       "1        110        120        130        140                              15   \n",
       "\n",
       "   type_21_programmable_rear_to_rear_headway_bin type_21_program_id  \\\n",
       "0                                           3000                  2   \n",
       "1                                           3000                  2   \n",
       "\n",
       "   type_10_vehicle_classification_scheme_primary  \\\n",
       "0                                              8   \n",
       "1                                              8   \n",
       "\n",
       "   type_10_vehicle_classification_scheme_secondary  \\\n",
       "0                                                5   \n",
       "1                                                5   \n",
       "\n",
       "   type_10_maximum_gap_milliseconds  type_10_maximum_differential_speed  \\\n",
       "0                              3000                                  20   \n",
       "1                              3000                                  20   \n",
       "\n",
       "   type_30_summary_interval_minutes  type_70_summary_interval_minutes  \\\n",
       "0                                15                                15   \n",
       "1                                15                                15   \n",
       "\n",
       "   type_70_vehicle_classification_scheme  type_70_maximum_gap_milliseconds  \\\n",
       "0                                     15                              3000   \n",
       "1                                     15                              3000   \n",
       "\n",
       "   type_70_maximum_differential_speed  type_70_error_bin_code  \\\n",
       "0                                  20                       1   \n",
       "1                                  20                       1   \n",
       "\n",
       "       start_datetime            end_datetime instrumentation_description  \\\n",
       "0 2020-02-01 00:00:00 2020-02-06 12:36:23.700                00001 RT8000   \n",
       "1 2020-02-06 12:37:47 2020-02-29 02:40:00.000                00001 RT8000   \n",
       "\n",
       "                              header_id  \n",
       "0  40a2f39a-2b0d-48b0-ba99-8dee58fba495  \n",
       "1  db7020ae-e86f-420d-8a4d-c10fb41dc91c  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[(df[0] == \"10\") & (df[3].isin([\"1\", \"0\", \"2\", \"3\", \"4\"]))].dropna(\n",
    "    axis=1, how=\"all\"\n",
    ")\n",
    "dfh2 = pd.DataFrame(df.loc[(df[0].isin([\"S0\", \"L1\"]))]).dropna(\n",
    "    axis=1, how=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\type10_data_insert.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/type10_data_insert.ipynb#ch0000009?line=53'>54</a>\u001b[0m                     sub_data_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([sub_data_df, tempdf])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/type10_data_insert.ipynb#ch0000009?line=54'>55</a>\u001b[0m                     col \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/type10_data_insert.ipynb#ch0000009?line=56'>57</a>\u001b[0m sub_data_df \u001b[39m=\u001b[39m sub_data_df\u001b[39m.\u001b[39mmerge(ddf[[\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m]], how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['id'] not in index\""
     ]
    }
   ],
   "source": [
    "if data.empty:\n",
    "    print(\"data empty\")\n",
    "    print(data)\n",
    "else:\n",
    "    num_of_fields = int(data.iloc[:,1].unique()[0])\n",
    "    ddf = data.iloc[:,: 2 + num_of_fields]\n",
    "    ddf.reset_index(inplace=True)\n",
    "\n",
    "    cols = ['index']\n",
    "    for i in range(ddf.shape[1]-1):\n",
    "        cols.append(config.TYPE10_DATA_COLUMN_NAMES[i])\n",
    "    ddf = pd.DataFrame(ddf.values, columns=cols)\n",
    "    ddf[\"data_id\"] = ddf.apply(lambda x: uuid.uuid4(), axis=1)\n",
    "\n",
    "    if data.shape[1] > ddf.shape[1]:\n",
    "        sub_data_df = pd.DataFrame(columns=['index','sub_data_type_code','offset_sensor_detection_code','mass_measurement_resolution_kg', 'number','value'])\n",
    "        for index, row in data.iterrows():\n",
    "            col = int(row[1]) + 2\n",
    "            while col < len(row) and row[col] != None:\n",
    "                sub_data_type = row[col]\n",
    "                col += 1\n",
    "                NoOfType = int(row[col])        \n",
    "                col +=1\n",
    "                if sub_data_type[0].lower() in ['w','a','g']:\n",
    "                    odc = row[col]\n",
    "                    col += 1\n",
    "                    mmr = row[col]\n",
    "                    col +=1\n",
    "                    for i in range(0,NoOfType):\n",
    "                        tempdf = pd.DataFrame([[index,\n",
    "                        sub_data_type,\n",
    "                        odc,\n",
    "                        mmr,\n",
    "                        i + 1,\n",
    "                        row[col]]\n",
    "                        ], columns = ['index',\n",
    "                        'sub_data_type_code',\n",
    "                        'offset_sensor_detection_code',\n",
    "                        'mass_measurement_resolution_kg',\n",
    "                        'number',\n",
    "                        'value'\n",
    "                        ])\n",
    "                        sub_data_df = pd.concat([sub_data_df, tempdf])\n",
    "                        col += 1\n",
    "                else:\n",
    "                    for i in range(0,NoOfType):\n",
    "                        tempdf = pd.DataFrame([[index, \n",
    "                        sub_data_type,\n",
    "                        i + 1,\n",
    "                        row[col]]], columns = ['index' ,\n",
    "                        'sub_data_type_code',\n",
    "                        'number',\n",
    "                        'value'])\n",
    "                        sub_data_df = pd.concat([sub_data_df, tempdf])\n",
    "                        col += 1\n",
    "\n",
    "    sub_data_df = sub_data_df.merge(ddf[['index', 'id']], how='left', on='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.fillna(0)\n",
    "ddf[\"assigned_lane_number\"] = ddf[\"assigned_lane_number\"].astype(int)\n",
    "max_lanes = ddf[\"assigned_lane_number\"].max()\n",
    "try:\n",
    "    ddf[\"direction\"] = ddf.apply(\n",
    "    lambda x: \"P\" if x[\"assigned_lane_number\"] <= (int(max_lanes) / 2) else \"N\",\n",
    "    axis=1,\n",
    ")\n",
    "    direction = dfh2.loc[dfh2[0] == \"L1\", 1:3]\n",
    "    direction = direction.drop_duplicates()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if ddf[\"departure_date\"].map(len).isin([8]).all():\n",
    "    ddf[\"start_datetime\"] = pd.to_datetime(\n",
    "        ddf[\"departure_date\"] + ddf[\"departure_time\"],\n",
    "        format=\"%Y%m%d%H%M%S%f\",\n",
    "    )\n",
    "elif ddf[\"departure_date\"].map(len).isin([6]).all():\n",
    "    ddf[\"start_datetime\"] = pd.to_datetime(\n",
    "        ddf[\"departure_date\"] + ddf[\"departure_time\"],\n",
    "        format=\"%y%m%d%H%M%S%f\",\n",
    "    )\n",
    "ddf['year'] = ddf['start_datetime'].dt.year\n",
    "t1 = dfh2.loc[dfh2[0] == \"S0\", 1].unique()\n",
    "ddf[\"site_id\"] = str(t1[0])\n",
    "ddf[\"site_id\"] = ddf[\"site_id\"].astype(str)\n",
    "ddf['departure_time'] = pd.to_datetime(ddf['departure_time'], format='%H%M%S%f')\n",
    "\n",
    "ddf = ddf.drop_duplicates()\n",
    "ddf[\"start_datetime\"] = ddf[\"start_datetime\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "scols = ddf.select_dtypes('object').columns\n",
    "\n",
    "ddf[scols] = ddf[scols].apply(pd.to_numeric, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE10_DATA_COL_LIST = [\"data_id\",\n",
    "\"site_id\",\n",
    "\"header_id\",\n",
    "\"year\",\n",
    "\"start_datetime\",\n",
    "\"direction\", \n",
    "\"number_of_fields_associated_with_the_basic_vehicle_data\",\n",
    "\"data_source_code\", \n",
    "\"edit_code\", \n",
    "\"departure_date\", \n",
    "\"departure_time\", \n",
    "\"assigned_lane_number\",\n",
    "\"physical_lane_number\",\n",
    "\"forward_reverse_code\",\n",
    "\"vehicle_category\",\n",
    "\"vehicle_class_code_primary_scheme\",\n",
    "\"vehicle_class_code_secondary_scheme\",\n",
    "\"vehicle_speed\",\n",
    "\"vehicle_length\",\n",
    "\"site_occupancy_time_in_milliseconds\",\n",
    "\"chassis_height_code\",\n",
    "\"vehicle_following_code\",\n",
    "\"vehicle_tag_code\",\n",
    "\"trailer_count\",\n",
    "\"axle_count\",\n",
    "\"bumper_to_1st_axle_spacing\",\n",
    "\"tyre_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf[ddf.columns.intersection(TYPE10_DATA_COL_LIST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.to_sql(\n",
    "    \"electronic_count_data_type_10\",\n",
    "    con=config.ENGINE,\n",
    "    schema=\"trafc\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    # method=psql_insert_copy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data_df = sub_data_df.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "sub_data_df = sub_data_df.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'w']\n",
    "sx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 's']\n",
    "gx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'g']\n",
    "vx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'v']\n",
    "tx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 't']\n",
    "ax_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'a']\n",
    "cx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MB2705851\\AppData\\Local\\Temp\\ipykernel_13148\\3861411644.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wx_data.rename(columns = {\"value\":\"wheel_mass\", \"number\":\"wheel_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
      "C:\\Users\\MB2705851\\AppData\\Local\\Temp\\ipykernel_13148\\3861411644.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sx_data.rename(columns = {\"value\":\"axle_spacing_cm\", \"number\":\"axle_spacing_number\", \"id\":\"type10_id\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "wx_data.rename(columns = {\"value\":\"wheel_mass\", \"number\":\"wheel_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "sx_data.rename(columns = {\"value\":\"axle_spacing_cm\", \"number\":\"axle_spacing_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "sx_data = sx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = wx_data.replace(r'^\\s*$', np.NaN, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wx_data.empty:\n",
    "    pass\n",
    "else:\n",
    "    wx_data.to_sql(\n",
    "    \"traffic_e_type10_wheel_mass\",\n",
    "    con=config.ENGINE,\n",
    "    schema=\"trafc\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=psql_insert_copy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sx_data.empty:\n",
    "    pass\n",
    "else:\n",
    "    sx_data.to_sql(\n",
    "    \"traffic_e_type10_axle_spacing\",\n",
    "    con=config.ENGINE,\n",
    "    schema=\"trafc\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=psql_insert_copy,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d86798f8e6233a76aa38644d13f9f426c72f43ddd8d5316bb9701fa2c43845d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
