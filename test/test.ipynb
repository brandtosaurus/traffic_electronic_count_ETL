{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import zipfile\n",
    "import traceback\n",
    "from numpy import empty\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import rsa_data as rd\n",
    "import rsa_headers as rh\n",
    "import config\n",
    "import queries as q\n",
    "import tools\n",
    "import gc\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLECTING FILES......\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\utf_8.py:15\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m### Codec APIs\u001b[39;00m\n\u001b[0;32m     13\u001b[0m encode \u001b[39m=\u001b[39m codecs\u001b[39m.\u001b[39mutf_8_encode\n\u001b[1;32m---> 15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39minput\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mutf_8_decode(\u001b[39minput\u001b[39m, errors, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIncrementalEncoder\u001b[39;00m(codecs\u001b[39m.\u001b[39mIncrementalEncoder):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files = getfiles(PATH)\n",
    "for file in files:\n",
    "    df = to_df(file)\n",
    "    dfh = get_head(df)\n",
    "    header = headers(dfh)\n",
    "    data = dtype21(df)\n",
    "    data = data_join(data, header)\n",
    "    data.drop(\"station_name\", axis=1, inplace=True)\n",
    "    for index, row in data.iterrows():\n",
    "        insert_qry = data_insert_type21(row)\n",
    "        update_qry = data_update_type21(row)\n",
    "        try:\n",
    "            with config.ENGINE.connect() as conn:\n",
    "                conn.execute(insert_qry)\n",
    "        except:\n",
    "            with config.ENGINE.connect() as conn:\n",
    "                conn.execute(update_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = r\"C:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\Manuals & Guidelines\\Traffic\\example data\\1114-20200229-240000.RSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(files: str):\n",
    "    try:\n",
    "        df = tools.to_df(files)\n",
    "\n",
    "        header = tools.get_head(df)\n",
    "        header = tools.header(header)\n",
    "        header[\"document_url\"] = str(files)\n",
    "\n",
    "\n",
    "        data = tools.dtype21(df)\n",
    "        data = tools.data_join(data, header)\n",
    "        data.drop(\"station_name\", axis=1, inplace=True)\n",
    "\n",
    "        d2 = tools.dtype30(df)\n",
    "        if d2 is None:\n",
    "            pass\n",
    "        else:\n",
    "            d2 = tools.data_join(d2, header)\n",
    "            data = data.merge(\n",
    "                d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "            )\n",
    "\n",
    "        d2 = tools.dtype70(df)\n",
    "        if d2 is None:\n",
    "            pass\n",
    "        else:\n",
    "            data = tools.data_join(d2, header)\n",
    "            data.drop(\"station_name\", axis=1, inplace=True)\n",
    "            data[\"start_datetime\"] = data[\"start_datetime\"].astype(\"datetime64[ns]\")\n",
    "            d2[\"start_datetime\"] = d2[\"start_datetime\"].astype(\"datetime64[ns]\")\n",
    "            data = data.merge(\n",
    "                d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "            )\n",
    "\n",
    "        d3, sub_data_df = tools.dtype10(df)\n",
    "        if d3 is None:\n",
    "            pass\n",
    "        else:\n",
    "            data = tools.data_join(d3, header)\n",
    "            tools.push_to_db(d3,\n",
    "            \"electronic_count_data_type_10\",\n",
    "            [\"site_id\", \"start_datetime\", \"assigned_lane_number\"],\n",
    "            )\n",
    "\n",
    "            sub_data_df = sub_data_df.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "            sub_data_df = sub_data_df.drop(\"index\", axis=1) \n",
    "            wx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'w']\n",
    "            sx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 's']\n",
    "            gx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'g']\n",
    "            vx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'v']\n",
    "            tx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 't']\n",
    "            ax_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'a']\n",
    "            cx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'c']\n",
    "\n",
    "            if wx_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                wx_data.rename(columns = {\"value\":\"wheel_mass\", \"number\":\"wheel_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "                # wx_data.to_sql(\n",
    "                #     \"traffic_e_type10_wheel_mass\",\n",
    "                #     con=config.ENGINE,\n",
    "                #     schema=\"trafc\",\n",
    "                #     if_exists=\"append\",\n",
    "                #     index=False,\n",
    "                #     method=tools.psql_insert_copy,\n",
    "                # )\n",
    "\n",
    "            if ax_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                ax_data.rename(columns = {\"value\":\"axle_mass\", \"number\":\"axle_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "            #     ax_data.to_sql(\n",
    "            #         \"traffic_e_type10_axle_mass\",\n",
    "            #         con=config.ENGINE,\n",
    "            #         schema=\"trafc\",\n",
    "            #         if_exists=\"append\",\n",
    "            #         index=False,\n",
    "            #         method=tools.psql_insert_copy,\n",
    "            #     )\n",
    "\n",
    "            if gx_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                gx_data.rename(columns = {\"value\":\"axle_group_mass\", \"number\":\"axle_group_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "            #     gx_data.to_sql(\n",
    "            #         \"traffic_e_type10_axle_group_mass\",\n",
    "            #         con=config.ENGINE,\n",
    "            #         schema=\"trafc\",\n",
    "            #         if_exists=\"append\",\n",
    "            #         index=False,\n",
    "            #         method=tools.psql_insert_copy,\n",
    "            #     )\n",
    "\n",
    "            if sx_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                sx_data.rename(columns = {\"value\":\"axle_spacing_cm\", \"number\":\"axle_spacing_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "                sx_data = sx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)\n",
    "            #     sx_data.to_sql(\n",
    "            #         \"traffic_e_type10_axle_spacing\",\n",
    "            #         con=config.ENGINE,\n",
    "            #         schema=\"trafc\",\n",
    "            #         if_exists=\"append\",\n",
    "            #         index=False,\n",
    "            #         method=tools.psql_insert_copy,\n",
    "            #     )\n",
    "\n",
    "            if tx_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                tx_data.rename(columns = {\"value\":\"tyre_code\", \"number\":\"tyre_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "                tx_data = tx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)\n",
    "            #     tx_data.to_sql(\n",
    "            #         \"traffic_e_type10_tyre\",\n",
    "            #         con=config.ENGINE,\n",
    "            #         schema=\"trafc\",\n",
    "            #         if_exists=\"append\",\n",
    "            #         index=False,\n",
    "            #         method=tools.psql_insert_copy,\n",
    "            #     )\n",
    "\n",
    "            if cx_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                cx_data.rename(columns = {\"value\":\"group_axle_count\", \"number\":\"group_axle_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "                cx_data = cx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)\n",
    "            #     cx_data.to_sql(\n",
    "            #         \"traffic_e_type10_axle_group_configuration\",\n",
    "            #         con=config.ENGINE,\n",
    "            #         schema=\"trafc\",\n",
    "            #         if_exists=\"append\",\n",
    "            #         index=False,\n",
    "            #         method=tools.psql_insert_copy,\n",
    "            #     )\n",
    "\n",
    "            if vx_data.empty:\n",
    "                pass\n",
    "            else:\n",
    "                vx_data.rename(columns = {\"value\":\"group_axle_count\", \"offset_sensor_detection_code\":\"vehicle_registration_number\" ,\"number\":\"group_axle_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "                vx_data = vx_data.drop([\"mass_measurement_resolution_kg\"], axis=1)\n",
    "            #     vx_data.to_sql(\n",
    "            #         \"traffic_e_type10_identification_data_images\",\n",
    "            #         con=config.ENGINE,\n",
    "            #         schema=\"trafc\",\n",
    "            #         if_exists=\"append\",\n",
    "            #         index=False,\n",
    "            #         method=tools.psql_insert_copy,\n",
    "            #     )\n",
    "\n",
    "        d2 = tools.dtype60(df)\n",
    "        if d2 is None:\n",
    "            pass\n",
    "        else:\n",
    "            data = tools.data_join(d2, header)\n",
    "            data.drop(\"station_name\", axis=1, inplace=True)\n",
    "            data = data.merge(\n",
    "                d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "            )\n",
    "\n",
    "        data = data.rename(columns=(lambda x: x[:-2] if '_x' in x else x))\n",
    "        header = header.rename(columns=(lambda x: x[:-2] if '_x' in x else x))\n",
    "\n",
    "        header = tools.header_calcs(header, data, 21)\n",
    "        header = tools.header_calcs(header, data, 30)\n",
    "        header = tools.header_calcs(header, data, 70)\n",
    "        header = tools.header_calcs(header, data, 60)\n",
    "\n",
    "        data = data[data.columns.intersection(config.DATA_COLUMN_NAMES)]\n",
    "        header = header[header.columns.intersection(config.HEADER_COLUMN_NAMES)]\n",
    "\n",
    "        # tools.push_to_partitioned_table(\n",
    "        #     data,\n",
    "        #     \"electronic_count_data_partitioned\",\n",
    "        #     [\"site_id\", \"start_datetime\", \"lane_number\"],\n",
    "        # )\n",
    "\n",
    "        # tools.push_to_db(\n",
    "        #     header,\n",
    "        #     \"electronic_count_header\",\n",
    "        #     [\"site_id\", \"start_datetime\", \"end_datetime\"],\n",
    "        # )\n",
    "\n",
    "        # data.to_csv(r\"C:\\Users\\MB2705851\\Desktop\\Temp\\rsa_traffic_counts\\data.csv\", index=False, mode='a')\n",
    "        # header.to_csv(r\"C:\\Users\\MB2705851\\Desktop\\Temp\\rsa_traffic_counts\\header.csv\", index=False, mode='a')\n",
    "\n",
    "    #     with open(\n",
    "    #         os.path.expanduser(config.FILES_COMPLETE),\n",
    "    #         \"a\",\n",
    "    #         newline=\"\",\n",
    "    #     ) as f:\n",
    "    #         write = csv.writer(f)\n",
    "    #         write.writerows([[files]])\n",
    "\n",
    "    except Exception as e:\n",
    "    #     print(e)\n",
    "    #     traceback.print_exc()\n",
    "    #     with open(\n",
    "    #         os.path.expanduser(config.PROBLEM_FILES),\n",
    "    #         \"a\",\n",
    "    #         newline=\"\",\n",
    "    #     ) as f:\n",
    "    #         write = csv.writer(f)\n",
    "    #         write.writerows([[files]])\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header = tools.get_head(df)\n",
    "header = tools.headers(header)\n",
    "header[\"document_url\"] = str(files)\n",
    "\n",
    "data = pd.DataFrame(colums=config.DATA_COLUMN_NAMES)\n",
    "\n",
    "d2 = tools.dtype21(df)\n",
    "if d2 is None:\n",
    "    pass\n",
    "else:\n",
    "    data = tools.data_join(d2, header)\n",
    "    data.drop(\"station_name\", axis=1, inplace=True)\n",
    "    data = data.merge(\n",
    "        d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "    )\n",
    "\n",
    "d2 = tools.dtype30(df)\n",
    "if d2 is None:\n",
    "    pass\n",
    "else:\n",
    "    d2 = tools.data_join(d2, header)\n",
    "    data = data.merge(\n",
    "        d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "    )\n",
    "\n",
    "d2 = tools.dtype70(df)\n",
    "if d2 is None:\n",
    "    pass\n",
    "else:\n",
    "    data = tools.data_join(d2, header)\n",
    "    data.drop(\"station_name\", axis=1, inplace=True)\n",
    "    data[\"start_datetime\"] = data[\"start_datetime\"].astype(\"datetime64[ns]\")\n",
    "    d2[\"start_datetime\"] = d2[\"start_datetime\"].astype(\"datetime64[ns]\")\n",
    "    data = data.merge(\n",
    "        d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "    )\n",
    "\n",
    "d2, sub_data_df = tools.dtype10(df)\n",
    "if d2 is None:\n",
    "    pass\n",
    "else:\n",
    "    data = tools.data_join(d2, header)\n",
    "    tools.push_to_db(d2,\n",
    "    \"electronic_count_data_type_10\",\n",
    "    [\"site_id\", \"start_datetime\", \"assigned_lane_number\"],\n",
    "    )\n",
    "\n",
    "    sub_data_df = sub_data_df.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "    sub_data_df = sub_data_df.drop(\"index\", axis=1) \n",
    "    wx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'w']\n",
    "    sx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 's']\n",
    "    gx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'g']\n",
    "    vx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'v']\n",
    "    tx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 't']\n",
    "    ax_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'a']\n",
    "    cx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'c']\n",
    "\n",
    "    if wx_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        wx_data.rename(columns = {\"value\":\"wheel_mass\", \"number\":\"wheel_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "        # wx_data.to_sql(\n",
    "        #     \"traffic_e_type10_wheel_mass\",\n",
    "        #     con=config.ENGINE,\n",
    "        #     schema=\"trafc\",\n",
    "        #     if_exists=\"append\",\n",
    "        #     index=False,\n",
    "        #     method=tools.psql_insert_copy,\n",
    "        # )\n",
    "\n",
    "    if ax_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        ax_data.rename(columns = {\"value\":\"axle_mass\", \"number\":\"axle_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "    #     ax_data.to_sql(\n",
    "    #         \"traffic_e_type10_axle_mass\",\n",
    "    #         con=config.ENGINE,\n",
    "    #         schema=\"trafc\",\n",
    "    #         if_exists=\"append\",\n",
    "    #         index=False,\n",
    "    #         method=tools.psql_insert_copy,\n",
    "    #     )\n",
    "\n",
    "    if gx_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        gx_data.rename(columns = {\"value\":\"axle_group_mass\", \"number\":\"axle_group_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "    #     gx_data.to_sql(\n",
    "    #         \"traffic_e_type10_axle_group_mass\",\n",
    "    #         con=config.ENGINE,\n",
    "    #         schema=\"trafc\",\n",
    "    #         if_exists=\"append\",\n",
    "    #         index=False,\n",
    "    #         method=tools.psql_insert_copy,\n",
    "    #     )\n",
    "\n",
    "    if sx_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        sx_data.rename(columns = {\"value\":\"axle_spacing_cm\", \"number\":\"axle_spacing_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "        sx_data = sx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)\n",
    "    #     sx_data.to_sql(\n",
    "    #         \"traffic_e_type10_axle_spacing\",\n",
    "    #         con=config.ENGINE,\n",
    "    #         schema=\"trafc\",\n",
    "    #         if_exists=\"append\",\n",
    "    #         index=False,\n",
    "    #         method=tools.psql_insert_copy,\n",
    "    #     )\n",
    "\n",
    "    if tx_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        tx_data.rename(columns = {\"value\":\"tyre_code\", \"number\":\"tyre_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "        tx_data = tx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)\n",
    "    #     tx_data.to_sql(\n",
    "    #         \"traffic_e_type10_tyre\",\n",
    "    #         con=config.ENGINE,\n",
    "    #         schema=\"trafc\",\n",
    "    #         if_exists=\"append\",\n",
    "    #         index=False,\n",
    "    #         method=tools.psql_insert_copy,\n",
    "    #     )\n",
    "\n",
    "    if cx_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        cx_data.rename(columns = {\"value\":\"group_axle_count\", \"number\":\"group_axle_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "        cx_data = cx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)\n",
    "    #     cx_data.to_sql(\n",
    "    #         \"traffic_e_type10_axle_group_configuration\",\n",
    "    #         con=config.ENGINE,\n",
    "    #         schema=\"trafc\",\n",
    "    #         if_exists=\"append\",\n",
    "    #         index=False,\n",
    "    #         method=tools.psql_insert_copy,\n",
    "    #     )\n",
    "\n",
    "    if vx_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        vx_data.rename(columns = {\"value\":\"group_axle_count\", \"offset_sensor_detection_code\":\"vehicle_registration_number\" ,\"number\":\"group_axle_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "        vx_data = vx_data.drop([\"mass_measurement_resolution_kg\"], axis=1)\n",
    "    #     vx_data.to_sql(\n",
    "    #         \"traffic_e_type10_identification_data_images\",\n",
    "    #         con=config.ENGINE,\n",
    "    #         schema=\"trafc\",\n",
    "    #         if_exists=\"append\",\n",
    "    #         index=False,\n",
    "    #         method=tools.psql_insert_copy,\n",
    "    #     )\n",
    "\n",
    "d2 = tools.dtype60(df)\n",
    "if d2 is None:\n",
    "    pass\n",
    "else:\n",
    "    data = tools.data_join(d2, header)\n",
    "    data.drop(\"station_name\", axis=1, inplace=True)\n",
    "    data = data.merge(\n",
    "        d2, how=\"outer\", on=[\"site_id\", \"start_datetime\", \"lane_number\"]\n",
    "    )\n",
    "\n",
    "data = data.rename(columns=(lambda x: x[:-2] if '_x' in x else x))\n",
    "header = header.rename(columns=(lambda x: x[:-2] if '_x' in x else x))\n",
    "\n",
    "header = tools.header_calcs(header, data, 21)\n",
    "header = tools.header_calcs(header, data, 30)\n",
    "header = tools.header_calcs(header, data, 70)\n",
    "header = tools.header_calcs(header, data, 60)\n",
    "\n",
    "data = data[data.columns.intersection(config.DATA_COLUMN_NAMES)]\n",
    "header = header[header.columns.intersection(config.HEADER_COLUMN_NAMES)]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f0240ba5a37eeffe7d380595dcf16248afe1beab2227011f0d1b1f050a1a57d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
