{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rsa_data as rd\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from io import StringIO\n",
    "import csv\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "src = r\"S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\"\n",
    "test1 = r\"S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\0006-20211231.RSV\"\n",
    "problem_files = [\n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0263-20220228.RSV', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE10_DATA_COLUMN_NAMES = ['site_id', 'header_id', \"year\", 'number_of_fields_associated_with_the_basic_vehicle_data', 'data_source_code', 'edit_code', 'departure_date', 'departure_time', 'assigned_lane_number', 'physical_lane_number', 'forward_reverse_code', 'vehicle_category', 'vehicle_class_code_primary_scheme', 'vehicle_class_code_secondary_scheme', 'vehicle_speed', 'vehicle_length', 'site_occupancy_time_in_milliseconds', 'chassis_height_code', 'vehicle_following_code', 'vehicle_tag_code', 'trailer_count', 'axle_count', 'bumper_to_1st_axle_spacing', 'tyre_type', 'sub_data_type_code_vx', 'vehicle_registration_number', 'number_of_images', 'image_name_1', 'image_name_2', 'image_name_3', \n",
    "'sub_data_type_code_sx', 'number_of_axles_spacings_counted', 'axle_spacing_1_between_individual_axles_cm', 'axle_spacing_2_between_individual_axles_cm', 'axle_spacing_3_between_individual_axles_cm', 'axle_spacing_4_between_individual_axles_cm', 'axle_spacing_5_between_individual_axles_cm', 'axle_spacing_6_between_individual_axles_cm', 'axle_spacing_7_between_individual_axles_cm', 'axle_spacing_8_between_individual_axles_cm',\n",
    "'start_datetime', 'direction', 'forward_direction_code', \n",
    "'sub_data_type_code_wx', 'number_of_wheel_masses', 'offset_sensor_detesction_code', 'mass_measurement_resolution', 'wheel_mass_for_wheel_1', 'wheel_mass_for_wheel_2', 'wheel_mass_for_wheel_3', 'wheel_mass_for_wheel_4', 'wheel_mass_for_wheel_5', 'wheel_mass_for_wheel_6', 'wheel_mass_for_wheel_7', 'wheel_mass_for_wheel_8', 'wheel_mass_for_wheel_9', 'wheel_mass_for_wheel_10'\n",
    "]\n",
    "\n",
    "TYPE10_HEADER_COLUMN_NAMES = ['header_id', 'data_description', 'vehicle_classification_scheme_primary', 'vehicle_classification_scheme_secondary', 'maximum_gap_milliseconds', 'maximum_differential_speed'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfiles(path: str):\n",
    "    print(\"COLLECTING FILES......\")\n",
    "    src = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if (\n",
    "                name.endswith(\".RSA\")\n",
    "                or name.endswith(\".rsa\")\n",
    "                or name.endswith(\".rsv\")\n",
    "                or name.endswith(\".RSV\")\n",
    "            ):\n",
    "                p = os.path.join(root, name)\n",
    "                src.append(p)\n",
    "    src = list(set(src))\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file, header=None, sep=\" \", low_memory=False)\n",
    "    df = df[0].str.split(\"\\s+|,\\s+|,\", expand=True)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "def push_to_db(df, table, subset) -> None:\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "    except Exception:\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = \", \".join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = \"{}.{}\".format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = \"COPY {} ({}) FROM STDIN WITH CSV\".format(table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "def join(header: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    if data.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        q = \"\"\"\n",
    "\t\tSELECT header.header_id, header.station_name, data.*\n",
    "\t\tFROM header\n",
    "\t\tLEFT JOIN data ON data.start_datetime WHERE data.start_datetime >= header.start_datetime AND data.end_datetime <= header.end_datetime;\n",
    "\t\t\"\"\"\n",
    "        q2 = \"\"\"UPDATE data set header_id = (SELECT header_id from header WHERE data.start_datetime >= header.start_datetime AND data.counttime_end <= header.enddate)\"\"\"\n",
    "        pysqldf = lambda q: sqldf(q, globals())\n",
    "        df = sqldf(q, locals())\n",
    "        df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_join(data: pd.DataFrame, header: pd.DataFrame) -> pd.DataFrame:\n",
    "    if data is None:\n",
    "        pass\n",
    "    elif data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        data = pd.DataFrame(data)\n",
    "        data = join(header, data)\n",
    "    return data\n",
    "\n",
    "def get_direction(lane_number, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        filt = df[1] == lane_number\n",
    "        df = df.where(filt)\n",
    "        df = df[2].dropna()\n",
    "        df = int(df)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "donelist = [\n",
    "'S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\0597-20211231.RSV',\n",
    "'S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\0133-20220228.RSV',\n",
    "'S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\0133-20220131.RSV',\n",
    "'S:\\Michael Brandt\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\0597-20211231.RSV',\n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\n7-20220131.RSV', \n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x013-20220228.RSV', \n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x0b1-20211231.RSV', \n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\n3-20220228.RSV', \n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x009-20220228.RSV', \n",
    "'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x073-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x088-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\n3-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\n3-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x1b7-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x193-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\x1a3-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0013-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0131-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0123-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0009-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0073-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0108-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0123-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0123-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0337-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0313-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0323-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0108-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0127-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0313-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0323-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0700-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0006-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0353-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0072-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0350-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0141-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0353-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0009-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0200-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0323-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0400-20220222.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0313-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0013-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0073-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\1699-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0095-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0045-20220104.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0044-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0151-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0141-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0350-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0048-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0131-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0095-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0700-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0072-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0108-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0006-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0006-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0045-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0288-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\1738-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\1699-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0065-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0169-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0169-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0131-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0073-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0141-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0009-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\1738-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0013-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0041-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0353-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0288-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0127-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0161-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0041-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0072-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0065-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0288-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\1699-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0044-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0169-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0065-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0048-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0337-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\1738-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0200-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0350-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0041-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0151-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0337-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0700-20220228.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0151-20211231.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0200-20220131.RSV', 'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0095-20211231.RSV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head(df) -> pd.DataFrame:\n",
    "    dfh = pd.DataFrame(\n",
    "        df.loc[\n",
    "            (df[0].isin([\"H0\", \"S0\", \"I0\", \"S1\", \"D0\", \"D1\", \"D3\", \"L0\", \"L1\"]))\n",
    "            | (\n",
    "                (df[0].isin([\"21\", \"70\", \"30\", \"13\", \"60\"]))\n",
    "                & (~df[1].isin([\"0\", \"1\", \"2\", \"3\", \"4\"]))\n",
    "            )\n",
    "            | (\n",
    "                (df[0].isin([\"10\"]))\n",
    "                & (df[1].isin([\"1\", \"8\", \"5\", \"01\", \"08\", \"05\"]))\n",
    "            )\n",
    "        ]\n",
    "    ).dropna(axis=1, how=\"all\")\n",
    "    dfh[\"index\"] = dfh.index\n",
    "    breaks = dfh[\"index\"].diff() != 1\n",
    "    groups = breaks.cumsum()\n",
    "    dfh[\"newindex\"] = groups\n",
    "    dfh = dfh.set_index(\"newindex\")\n",
    "    dfh = dfh.drop(columns=[\"index\"])\n",
    "    return dfh\n",
    "\n",
    "def headers(dfh: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not dfh.empty:\n",
    "        headers = pd.DataFrame()\n",
    "        headers[\"site_id\"] = dfh.loc[dfh[0] == \"S0\", 1].astype(str)\n",
    "        if not dfh.loc[dfh[0] == \"S1\", 1:].empty:\n",
    "            headers[\"station_name\"] = (\n",
    "                dfh.loc[dfh[0] == \"S1\", 1:]\n",
    "                .dropna(axis=1)\n",
    "                .apply(\" \".join, axis=1)\n",
    "                .astype(str)\n",
    "            )\n",
    "        else:\n",
    "            headers[\"station_name\"] = (\n",
    "                dfh.loc[dfh[0] == \"S0\", 2:]\n",
    "                .dropna(axis=1)\n",
    "                .apply(\" \".join, axis=1)\n",
    "                .astype(str)\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            headers[\"y\"] = dfh.loc[dfh[0] == \"S0\", 5].astype(float)\n",
    "            headers[\"x\"] = dfh.loc[dfh[0] == \"S0\", 6].astype(float)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        headers[\"number_of_lanes\"] = dfh.loc[dfh[0] == \"L0\", 2].astype(int)\n",
    "\n",
    "        try:\n",
    "            headers[\"speedbin1\"] = dfh.loc[dfh[0] == \"21\", 4].astype(int)\n",
    "            headers[\"speedbin2\"] = dfh.loc[dfh[0] == \"21\", 5].astype(int)\n",
    "            headers[\"speedbin3\"] = dfh.loc[dfh[0] == \"21\", 6].astype(int)\n",
    "            headers[\"speedbin4\"] = dfh.loc[dfh[0] == \"21\", 7].astype(int)\n",
    "            headers[\"speedbin5\"] = dfh.loc[dfh[0] == \"21\", 8].astype(int)\n",
    "            headers[\"speedbin6\"] = dfh.loc[dfh[0] == \"21\", 9].astype(int)\n",
    "            headers[\"speedbin7\"] = dfh.loc[dfh[0] == \"21\", 10].astype(int)\n",
    "            headers[\"speedbin8\"] = dfh.loc[dfh[0] == \"21\", 11].astype(int)\n",
    "            headers[\"speedbin9\"] = dfh.loc[dfh[0] == \"21\", 12].astype(int)\n",
    "            headers[\"type_21_count_interval_minutes\"] = dfh.loc[\n",
    "                dfh[0] == \"21\", 1\n",
    "            ].astype(int)\n",
    "            headers[\"type_21_programmable_rear_to_rear_headway_bin\"] = dfh.loc[\n",
    "                dfh[0] == \"21\", 3\n",
    "            ].astype(int)\n",
    "            headers[\"type_21_program_id\"] = \"2\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_10_vehicle_classification_scheme_primary\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 1\n",
    "            ].astype(int)\n",
    "            headers[\"type_10_vehicle_classification_scheme_secondary\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 2\n",
    "            ].astype(int)\n",
    "            headers[\"type_10_maximum_gap_milliseconds\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 3\n",
    "            ].astype(int)\n",
    "            headers[\"type_10_maximum_differential_speed\"] = dfh.loc[\n",
    "                dfh[0] == \"10\", 4\n",
    "            ].astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_30_summary_interval_minutes\"] = dfh.loc[\n",
    "                dfh[0] == \"30\", 2\n",
    "            ].astype(int)\n",
    "            headers[\"type_30_vehicle_classification_scheme\"] = dfh.loc[\n",
    "                dfh[0] == \"30\", 3\n",
    "            ].astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_70_summary_interval_minutes\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 1\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_vehicle_classification_scheme\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 2\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_maximum_gap_milliseconds\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 3\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_maximum_differential_speed\"] = dfh.loc[\n",
    "                dfh[0] == \"70\", 4\n",
    "            ].astype(int)\n",
    "            headers[\"type_70_error_bin_code\"] = dfh.loc[dfh[0] == \"70\", 5].astype(\n",
    "                int\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not dfh.loc[dfh[0] == \"D3\", 1].empty:\n",
    "            headers[\"start_datetime\"] = dfh.loc[dfh[0] == \"D3\", 1].astype(str)\n",
    "            headers[\"start_time\"] = dfh.loc[dfh[0] == \"D3\", 2].astype(str)\n",
    "            headers[\"end_datetime\"] = dfh.loc[dfh[0] == \"D3\", 3].astype(str)\n",
    "            headers[\"end_time\"] = dfh.loc[dfh[0] == \"D3\", 4].astype(str)\n",
    "        else:\n",
    "            headers[\"start_datetime\"] = dfh.loc[dfh[0] == \"D1\", 1].astype(str)\n",
    "            headers[\"start_time\"] = dfh.loc[dfh[0] == \"D1\", 2].astype(str)\n",
    "            headers[\"end_datetime\"] = dfh.loc[dfh[0] == \"D1\", 3].astype(str)\n",
    "            headers[\"end_time\"] = dfh.loc[dfh[0] == \"D1\", 4].astype(str)\n",
    "\n",
    "        # headers[\"end_datetime\"] = headers.apply(\n",
    "        #     lambda x: pd.to_datetime(\n",
    "        #         x[\"end_datetime\"] + x[\"end_time\"], format=\"%y%m%d%H%M%S\"\n",
    "        #     )\n",
    "        #     if (\n",
    "        #         x[\"end_time\"] != \"240000\"\n",
    "        #         and len(x[\"end_datetime\"]) == 6\n",
    "        #         and len(x[\"end_time\"]) == 6\n",
    "        #     )\n",
    "        #     else (\n",
    "        #         pd.to_datetime(\n",
    "        #             x[\"end_datetime\"] + x[\"end_time\"], format=\"%y%m%d%H%M%S%f\"\n",
    "        #         )\n",
    "        #         if (\n",
    "        #             x[\"end_time\"] != \"24000000\"\n",
    "        #             and len(x[\"end_datetime\"]) == 6\n",
    "        #             and len(x[\"end_time\"]) == 8\n",
    "        #         )\n",
    "        #         else (\n",
    "        #             pd.to_datetime(\n",
    "        #                 x[\"end_datetime\"] + x[\"end_time\"], format=\"%Y%m%d%H%M%S\"\n",
    "        #             )\n",
    "        #             if (\n",
    "        #                 x[\"end_time\"] != \"240000\"\n",
    "        #                 and len(x[\"end_datetime\"]) == 8\n",
    "        #                 and len(x[\"end_time\"]) == 6\n",
    "        #             )\n",
    "        #             else (\n",
    "        #                 pd.to_datetime(\n",
    "        #                     x[\"end_datetime\"] + x[\"end_time\"],\n",
    "        #                     format=\"%Y%m%d%H%M%S%f\",\n",
    "                        # )\n",
    "                        # if (\n",
    "                        #     x[\"end_time\"] != \"24000000\"\n",
    "                        #     and len(x[\"end_datetime\"]) == 8\n",
    "                        #     and len(x[\"end_time\"]) == 8\n",
    "                        # )\n",
    "                        # else (\n",
    "                        #     pd.to_datetime(x[\"end_datetime\"], format=\"%y%m%d\")\n",
    "                        #     + timedelta(days=1)\n",
    "                        #     if (\n",
    "                        #         x[\"end_time\"] == \"240000\"\n",
    "                        #         and len(x[\"end_datetime\"]) == 6\n",
    "                            #     and len(x[\"end_time\"]) == 6\n",
    "                            # )\n",
    "                            # else (\n",
    "                            #     pd.to_datetime(x[\"end_datetime\"], format=\"%y%m%d\")\n",
    "                            #     + timedelta(days=1)\n",
    "                            #     if (\n",
    "                            #         x[\"end_time\"] == \"24000000\"\n",
    "                            #         and len(x[\"end_datetime\"]) == 6\n",
    "                            #         and len(x[\"end_time\"]) == 8\n",
    "                            #     )\n",
    "                            #     else (\n",
    "                            #         pd.to_datetime(\n",
    "                            #             x[\"end_datetime\"], format=\"%Y%m%d\"\n",
    "                            #         )\n",
    "                            #         + timedelta(days=1)\n",
    "                            #         if (\n",
    "                            #             x[\"end_time\"] == \"240000\"\n",
    "                                #         and len(x[\"end_datetime\"]) == 8\n",
    "                                #         and len(x[\"end_time\"]) == 6\n",
    "                                #     )\n",
    "                                #     else (\n",
    "                                #         pd.to_datetime(\n",
    "                                #             x[\"end_datetime\"], format=\"%Y%m%d\"\n",
    "                                #         )\n",
    "                                #         + timedelta(days=1)\n",
    "                                #         if (\n",
    "                                #             x[\"end_time\"] == \"24000000\"\n",
    "                                #             and len(x[\"end_datetime\"]) == 8\n",
    "                                #             and len(x[\"end_time\"]) == 8\n",
    "                                #         )\n",
    "                                #         else pd.to_datetime(\n",
    "                                #             x[\"end_datetime\"] + x[\"end_time\"]\n",
    "                                #         )\n",
    "                                #     )\n",
    "                                # )\n",
    "        #                     )\n",
    "        #                 )\n",
    "        #             )\n",
    "        #         )\n",
    "        #     ),\n",
    "        #     axis=1,\n",
    "        # )\n",
    "\n",
    "        # headers[\"start_datetime\"] = headers.apply(\n",
    "        #     lambda x: pd.to_datetime(\n",
    "        #         x[\"start_datetime\"] + x[\"start_time\"], format=\"%y%m%d%H%M%S\"\n",
    "        #     )\n",
    "        #     if (\n",
    "        #         x[\"start_time\"] != \"240000\"\n",
    "        #         and len(x[\"start_datetime\"]) == 6\n",
    "        #         and len(x[\"start_time\"]) == 6\n",
    "        #     )\n",
    "        #     else (\n",
    "        #         pd.to_datetime(\n",
    "        #             x[\"start_datetime\"] + x[\"start_time\"], format=\"%y%m%d%H%M%S%f\"\n",
    "        #         )\n",
    "        #         if (\n",
    "        #             x[\"start_time\"] != \"24000000\"\n",
    "        #             and len(x[\"start_datetime\"]) == 6\n",
    "        #             and len(x[\"start_time\"]) == 8\n",
    "        #         )\n",
    "        #         else (\n",
    "        #             pd.to_datetime(\n",
    "        #                 x[\"start_datetime\"] + x[\"start_time\"], format=\"%Y%m%d%H%M%S\"\n",
    "        #             )\n",
    "        #             if (\n",
    "        #                 x[\"start_time\"] != \"240000\"\n",
    "        #                 and len(x[\"start_datetime\"]) == 8\n",
    "                    #     and len(x[\"start_time\"]) == 6\n",
    "                    # )\n",
    "                    # else (\n",
    "                    #     pd.to_datetime(\n",
    "                    #         x[\"start_datetime\"] + x[\"start_time\"],\n",
    "                    #         format=\"%Y%m%d%H%M%S%f\",\n",
    "                    #     )\n",
    "                    #     if (\n",
    "                    #         x[\"start_time\"] != \"24000000\"\n",
    "                    #         and len(x[\"start_datetime\"]) == 8\n",
    "                    #         and len(x[\"start_time\"]) == 8\n",
    "                    #     )\n",
    "                    #     else (\n",
    "                    #         pd.to_datetime(x[\"start_datetime\"], format=\"%y%m%d\")\n",
    "                    #         + timedelta(days=1)\n",
    "                    #         if (\n",
    "                    #             x[\"start_time\"] == \"240000\"\n",
    "                    #             and len(x[\"start_datetime\"]) == 6\n",
    "                    #             and len(x[\"start_time\"]) == 6\n",
    "                    #         )\n",
    "                            # else (\n",
    "                            #     pd.to_datetime(x[\"start_datetime\"], format=\"%y%m%d\")\n",
    "                            #     + timedelta(days=1)\n",
    "                            #     if (\n",
    "                            #         x[\"start_time\"] == \"24000000\"\n",
    "                            #         and len(x[\"start_datetime\"]) == 6\n",
    "                            #         and len(x[\"start_time\"]) == 8\n",
    "                            #     )\n",
    "                            #     else (\n",
    "                            #         pd.to_datetime(\n",
    "                            #             x[\"start_datetime\"], format=\"%Y%m%d\"\n",
    "                            #         )\n",
    "                            #         + timedelta(days=1)\n",
    "                            #         if (\n",
    "                            #             x[\"start_time\"] == \"240000\"\n",
    "                            #             and len(x[\"start_datetime\"]) == 8\n",
    "                            #             and len(x[\"start_time\"]) == 6\n",
    "                            #         )\n",
    "        #                             else (\n",
    "        #                                 pd.to_datetime(\n",
    "        #                                     x[\"start_datetime\"], format=\"%Y%m%d\"\n",
    "        #                                 )\n",
    "        #                                 + timedelta(days=1)\n",
    "        #                                 if (\n",
    "        #                                     x[\"start_time\"] == \"24000000\"\n",
    "        #                                     and len(x[\"start_datetime\"]) == 8\n",
    "        #                                     and len(x[\"start_time\"]) == 8\n",
    "        #                                 )\n",
    "        #                                 else pd.to_datetime(\n",
    "        #                                     x[\"start_datetime\"] + x[\"start_time\"]\n",
    "        #                                 )\n",
    "        #                             )\n",
    "        #                         )\n",
    "        #                     )\n",
    "        #                 )\n",
    "        #             )\n",
    "        #         )\n",
    "        #     ),\n",
    "        #     axis=1,\n",
    "        # )\n",
    "\n",
    "        # headers = headers.drop([\"start_time\"], axis=1)\n",
    "        # headers = headers.drop([\"end_time\"], axis=1)\n",
    "\n",
    "        # headers[\"start_datetime\"] = pd.to_datetime(headers[\"start_datetime\"])\n",
    "        # headers[\"end_datetime\"] = pd.to_datetime(headers[\"end_datetime\"])\n",
    "        headers[\"site_id\"] = headers[\"site_id\"].astype(str)\n",
    "\n",
    "        try:\n",
    "            headers[\"instrumentation_description\"] = (\n",
    "                dfh.loc[dfh[0] == \"I0\", 1:]\n",
    "                .dropna(axis=1)\n",
    "                .apply(\" \".join, axis=1)\n",
    "                .astype(str)\n",
    "            )\n",
    "        except Exception:\n",
    "            headers[\"instrumentation_description\"] = None\n",
    "\n",
    "        try:\n",
    "            headers[\"type_30_summary_interval_minutes\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            headers[\"type_70_summary_interval_minutes\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "            headers[\"type_70_vehicle_classification_scheme\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "            headers[\"type_70_vehicle_classification_scheme\"] = headers[\n",
    "                \"type_21_count_interval_minutes\"\n",
    "            ]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        headers = headers.fillna(method=\"ffill\")\n",
    "        headers = headers.fillna(method=\"bfill\")\n",
    "\n",
    "        headers = headers.drop_duplicates(ignore_index=True)\n",
    "\n",
    "        headers[\"header_id\"] = \"\"\n",
    "        headers[\"header_id\"] = headers[\"header_id\"].apply(\n",
    "            lambda x: str(uuid.uuid4())\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    return headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_df(r'S:\\\\Michael Brandt\\\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22\\\\0263-20220228.RSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE10_DATA_COLUMN_NAMES = [\n",
    "\"data_type_code\",\n",
    "\"number_of_fields_associated_with_the_basic_vehicle_data\",\n",
    "\"data_source_code\",\n",
    "\"edit_code\",\n",
    "\"departure_date\",\n",
    "\"departure_time\",\n",
    "\"assigned_lane_number\",\n",
    "\"physical_lane_number\",\n",
    "\"forward_reverse_code\",\n",
    "\"vehicle_category\",\n",
    "\"vehicle_class_code_primary_scheme\",\n",
    "\"vehicle_class_code_secondary_scheme\",\n",
    "\"vehicle_speed\",\n",
    "\"vehicle_length\",\n",
    "\"site_occupancy_time_in_milliseconds\",\n",
    "\"chassis_height_code\",\n",
    "\"vehicle_following_code\",\n",
    "\"vehicle_tag_code\",\n",
    "\"trailer_count\",\n",
    "\"axle_count\",\n",
    "\"bumper_to_1st_axle_spacing\",\n",
    "\"tyre_type\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[(df[0] == \"10\") & (df[3].isin([\"1\", \"0\"]))].dropna(\n",
    "    axis=1, how=\"all\"\n",
    ")\n",
    "dfh2 = pd.DataFrame(df.loc[(df[0].isin([\"S0\", \"L1\"]))]).dropna(\n",
    "    axis=1, how=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.empty:\n",
    "    print(\"data empty\")\n",
    "    print(data)\n",
    "else:\n",
    "    num_of_fields = int(data.iloc[:,1].unique()[0])\n",
    "    ddf = data.iloc[:,: 2 + num_of_fields]\n",
    "    ddf.reset_index(inplace=True)\n",
    "\n",
    "    cols = ['index']\n",
    "    for i in range(ddf.shape[1]-1):\n",
    "        cols.append(config.TYPE10_DATA_COLUMN_NAMES[i])\n",
    "    ddf = pd.DataFrame(ddf.values, columns=cols)\n",
    "    ddf[\"data_id\"] = ddf.apply(lambda x: uuid.uuid4(), axis=1)\n",
    "\n",
    "    if data.shape[1] > ddf.shape[1]:\n",
    "        sub_data_df = pd.DataFrame(columns=['index','sub_data_type_code','offset_sensor_detection_code','mass_measurement_resolution_kg', 'number','value'])\n",
    "        for index, row in data.iterrows():\n",
    "            col = int(row[1]) + 2\n",
    "            while col < len(row) and row[col] != None:\n",
    "                sub_data_type = row[col]\n",
    "                col += 1\n",
    "                NoOfType = int(row[col])        \n",
    "                col +=1\n",
    "                if sub_data_type[0].lower() in ['w','a','g']:\n",
    "                    odc = row[col]\n",
    "                    col += 1\n",
    "                    mmr = row[col]\n",
    "                    col +=1\n",
    "                    for i in range(0,NoOfType):\n",
    "                        tempdf = pd.DataFrame([[index,\n",
    "                        sub_data_type,\n",
    "                        odc,\n",
    "                        mmr,\n",
    "                        i + 1,\n",
    "                        row[col]]\n",
    "                        ], columns = ['index',\n",
    "                        'sub_data_type_code',\n",
    "                        'offset_sensor_detection_code',\n",
    "                        'mass_measurement_resolution_kg',\n",
    "                        'number',\n",
    "                        'value'\n",
    "                        ])\n",
    "                        sub_data_df = pd.concat([sub_data_df, tempdf])\n",
    "                        col += 1\n",
    "                else:\n",
    "                    for i in range(0,NoOfType):\n",
    "                        tempdf = pd.DataFrame([[index, \n",
    "                        sub_data_type,\n",
    "                        i + 1,\n",
    "                        row[col]]], columns = ['index' ,\n",
    "                        'sub_data_type_code',\n",
    "                        'number',\n",
    "                        'value'])\n",
    "                        sub_data_df = pd.concat([sub_data_df, tempdf])\n",
    "                        col += 1\n",
    "\n",
    "    sub_data_df = sub_data_df.merge(ddf[['index', 'id']], how='left', on='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.fillna(0)\n",
    "ddf[\"assigned_lane_number\"] = ddf[\"assigned_lane_number\"].astype(int)\n",
    "max_lanes = ddf[\"assigned_lane_number\"].max()\n",
    "try:\n",
    "    ddf[\"direction\"] = ddf.apply(\n",
    "    lambda x: \"P\" if x[\"assigned_lane_number\"] <= (int(max_lanes) / 2) else \"N\",\n",
    "    axis=1,\n",
    ")\n",
    "    direction = dfh2.loc[dfh2[0] == \"L1\", 1:3]\n",
    "    direction = direction.drop_duplicates()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if ddf[\"departure_date\"].map(len).isin([8]).all():\n",
    "    ddf[\"start_datetime\"] = pd.to_datetime(\n",
    "        ddf[\"departure_date\"] + ddf[\"departure_time\"],\n",
    "        format=\"%Y%m%d%H%M%S%f\",\n",
    "    )\n",
    "elif ddf[\"departure_date\"].map(len).isin([6]).all():\n",
    "    ddf[\"start_datetime\"] = pd.to_datetime(\n",
    "        ddf[\"departure_date\"] + ddf[\"departure_time\"],\n",
    "        format=\"%y%m%d%H%M%S%f\",\n",
    "    )\n",
    "ddf['year'] = ddf['start_datetime'].dt.year\n",
    "t1 = dfh2.loc[dfh2[0] == \"S0\", 1].unique()\n",
    "ddf[\"site_id\"] = str(t1[0])\n",
    "ddf[\"site_id\"] = ddf[\"site_id\"].astype(str)\n",
    "ddf['departure_time'] = pd.to_datetime(ddf['departure_time'], format='%H%M%S%f')\n",
    "\n",
    "ddf = ddf.drop_duplicates()\n",
    "ddf[\"start_datetime\"] = ddf[\"start_datetime\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "\n",
    "scols = ddf.select_dtypes('object').columns\n",
    "\n",
    "ddf[scols] = ddf[scols].apply(pd.to_numeric, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE10_DATA_COL_LIST = [\"data_id\",\n",
    "\"site_id\",\n",
    "\"header_id\",\n",
    "\"year\",\n",
    "\"start_datetime\",\n",
    "\"direction\", \n",
    "\"number_of_fields_associated_with_the_basic_vehicle_data\",\n",
    "\"data_source_code\", \n",
    "\"edit_code\", \n",
    "\"departure_date\", \n",
    "\"departure_time\", \n",
    "\"assigned_lane_number\",\n",
    "\"physical_lane_number\",\n",
    "\"forward_reverse_code\",\n",
    "\"vehicle_category\",\n",
    "\"vehicle_class_code_primary_scheme\",\n",
    "\"vehicle_class_code_secondary_scheme\",\n",
    "\"vehicle_speed\",\n",
    "\"vehicle_length\",\n",
    "\"site_occupancy_time_in_milliseconds\",\n",
    "\"chassis_height_code\",\n",
    "\"vehicle_following_code\",\n",
    "\"vehicle_tag_code\",\n",
    "\"trailer_count\",\n",
    "\"axle_count\",\n",
    "\"bumper_to_1st_axle_spacing\",\n",
    "\"tyre_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf[ddf.columns.intersection(TYPE10_DATA_COL_LIST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.to_sql(\n",
    "    \"electronic_count_data_type_10\",\n",
    "    con=config.ENGINE,\n",
    "    schema=\"trafc\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    # method=psql_insert_copy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data_df = sub_data_df.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "sub_data_df = sub_data_df.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'w']\n",
    "sx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 's']\n",
    "gx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'g']\n",
    "vx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'v']\n",
    "tx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 't']\n",
    "ax_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'a']\n",
    "cx_data = sub_data_df.loc[sub_data_df['sub_data_type_code'].str.lower().str[0] == 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MB2705851\\AppData\\Local\\Temp\\ipykernel_13148\\3861411644.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wx_data.rename(columns = {\"value\":\"wheel_mass\", \"number\":\"wheel_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
      "C:\\Users\\MB2705851\\AppData\\Local\\Temp\\ipykernel_13148\\3861411644.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sx_data.rename(columns = {\"value\":\"axle_spacing_cm\", \"number\":\"axle_spacing_number\", \"id\":\"type10_id\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "wx_data.rename(columns = {\"value\":\"wheel_mass\", \"number\":\"wheel_mass_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "sx_data.rename(columns = {\"value\":\"axle_spacing_cm\", \"number\":\"axle_spacing_number\", \"id\":\"type10_id\"}, inplace=True)\n",
    "sx_data = sx_data.drop([\"offset_sensor_detection_code\",\"mass_measurement_resolution_kg\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = wx_data.replace(r'^\\s*$', np.NaN, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wx_data.empty:\n",
    "    pass\n",
    "else:\n",
    "    wx_data.to_sql(\n",
    "    \"traffic_e_type10_wheel_mass\",\n",
    "    con=config.ENGINE,\n",
    "    schema=\"trafc\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=psql_insert_copy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sx_data.empty:\n",
    "    pass\n",
    "else:\n",
    "    sx_data.to_sql(\n",
    "    \"traffic_e_type10_axle_spacing\",\n",
    "    con=config.ENGINE,\n",
    "    schema=\"trafc\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=psql_insert_copy,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af8b1e165c1ff2d014348c231bd068624d4f3d6b612700b0d12a9816f8e61c96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
