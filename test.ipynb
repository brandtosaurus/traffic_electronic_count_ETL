{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "from datetime import timedelta, date\n",
    "import uuid\n",
    "\n",
    "# import rsa_data_summary as rd\n",
    "# import rsa_data_wim as wim\n",
    "# import rsa_headers as rh\n",
    "import config\n",
    "import queries as q\n",
    "import tools\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual tables for type 30 and 60 test\n",
    "# files = tools.getfiles(r'C:\\FTP\\Syntell\\SMEC RSA Files_GP PRM Sites_Dec21toFeb22')\n",
    "test_header = r\"C:\\PQ410\\2 - Data In\\TIS Data\\CSIR\\2010\\Secondary Loop\\0007.rsa\"\n",
    "file = r\"C:\\PQ410\\2 - Data In\\TIS Data\\GDRT\\GP_RSA 2014\\0038.RSA\"\n",
    "t21_file =r\"C:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\Manuals & Guidelines\\Traffic\\example data\\type21_1.RSA\"\n",
    "t10_file = r\"C:\\FTP\\Syntell\\SMEC RSV Files_GP PRM Sites_Dec21toFeb22_individuals\\0006-20211231.RSV\"\n",
    "t10_file2 = r\"C:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\Manuals & Guidelines\\Traffic\\example data\\type21_60_70.RSA\"\n",
    "type60_file = r\"C:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\Manuals & Guidelines\\Traffic\\example data\\type21_60_70.RSA\"\n",
    "new_test = r\"C:\\PQ410\\2 - Data In\\TIS Data\\Disks Sept 2020\\Data Discs\\2017\\2017Q2\\Historic\\GPG CTO 2011 Yearbook\\RSA Data\\0317.rsa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electronic_count_data_type_21_columns = list(pd.read_sql_query(\"SELECT * from trafc.electronic_count_data_type_21 limit 1\",config.ENGINE).columns)\n",
    "# electronic_count_data_type_30_columns = list(pd.read_sql_query(\"SELECT * from trafc.electronic_count_data_type_30 limit 1\",config.ENGINE).columns)\n",
    "electronic_count_data_type_60_columns = list(pd.read_sql_query(\"SELECT * from trafc.electronic_count_data_type_60 limit 1\",config.ENGINE).columns)\n",
    "# electronic_count_data_type_70_columns = list(pd.read_sql_query(\"SELECT * from trafc.electronic_count_data_type_70 limit 1\",config.ENGINE).columns)\n",
    "# header_columns = list(pd.read_sql_query(\"SELECT * from trafc.electronic_count_header limit 1\",config.ENGINE).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = tools.to_df(t10_file)\n",
    "dfa = pd.read_csv(t10_file2, header=None) #, sep='\\s+', engine='python')\n",
    "# dfa = pd.read_csv(t21_file, header=None, sep='\\s+', engine='python')\n",
    "\n",
    "dfa = dfa[0].str.split(\"\\s+|,\\s+|,|;|;\\s+\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dfa.loc[\n",
    "    (~dfa[0].isin([\"H0\", \"H9\", \"S0\", \"I0\", \"S1\", \"D0\", \"D1\", \"D3\", \"L0\", \"L1\"]))\n",
    "    & ((\n",
    "        (dfa[0].isin([\"21\", \"22\", \"70\", \"30\", \"31\", \"13\", \"60\"]))\n",
    "        & (dfa[1].isin([\"0\", \"1\", \"2\", \"3\", \"4\"]))\n",
    "        & (dfa.loc[dfa[0].isin([\"21\", \"70\", \"30\", \"13\", \"60\"])][3].astype(int) > 80)\n",
    "    ) | (\n",
    "        (dfa[0].isin([\"10\"]))\n",
    "        & (~dfa[1].isin([\"1\", \"8\", \"5\", \"9\", \"01\", \"08\", \"05\", \"09\"]))\n",
    "        & (dfa.loc[dfa[0].isin([\"10\"])][4].astype(int) > 80)\n",
    "    ))           \n",
    "    ]).dropna(axis=1, how=\"all\").reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_df = pd.DataFrame(dfa.loc[\n",
    "    (dfa[0].isin([\"H0\", \"S0\", \"I0\", \"S1\", \"D0\", \"D1\", \"D3\", \"L0\", \"L1\"]))\n",
    "    | (\n",
    "        (dfa[0].isin([\"21\", \"70\", \"30\", \"13\", \"60\"]))\n",
    "        & (dfa.loc[dfa[0].isin([\"21\", \"70\", \"30\", \"13\", \"60\"])][2].astype(int) < 80)\n",
    "    )\n",
    "    | (\n",
    "        (dfa[0].isin([\"10\"]))\n",
    "        & (dfa[1].isin([\"1\", \"8\", \"5\", \"9\", \"01\", \"08\", \"05\", \"09\"]))\n",
    "    )\n",
    "]).dropna(axis=1, how=\"all\").reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14        15\n",
       "15        15\n",
       "16        15\n",
       "18    120910\n",
       "19    120910\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa.loc[dfa[0].isin([\"21\", \"70\", \"30\", \"13\", \"60\"]),2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_col:2\n",
      "time_col:3\n",
      "6\n",
      "0    120910\n",
      "1    120910\n",
      "2    120910\n",
      "3    120910\n",
      "4    120910\n",
      "Name: 2, dtype: object\n",
      "0    2012-09-1012:30:00.000000\n",
      "1    2012-09-1012:30:00.000000\n",
      "2    2012-09-1012:30:00.000000\n",
      "3    2012-09-1012:30:00.000000\n",
      "4    2012-09-1012:30:00.000000\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "if df.loc[df[0]==\"10\"].empty:\n",
    "    duration_min = int(df[4].at[0])\n",
    "    date_col = 2\n",
    "    time_col = 3\n",
    "    add_day = 1\n",
    "    date_col_name = \"end_date\"\n",
    "    time_col_name = \"end_time\"\n",
    "    typ = \"sum\"\n",
    "else:\n",
    "    date_col = 4\n",
    "    time_col = 5\n",
    "    duration_min = 0\n",
    "    add_day = 0\n",
    "    date_col_name = \"departure_date\"\n",
    "    time_col_name = \"departure_time\"\n",
    "    typ = \"indv\"\n",
    "date_format = \"%Y-%m-%d\" # 2021-12-01\n",
    "time_format = \"%H:%M:%S.%f\" # 00:00:00.00\n",
    "    \n",
    "date_length = len(df[date_col].at[0])\n",
    "\n",
    "df[time_col] = df[time_col].astype(str)\n",
    "\n",
    "df[time_col] = df[time_col].str.pad(width=7,side='right',fillchar=\"0\")\n",
    "df[time_col].loc[df[time_col].str[:2] == '24'] = ('0').zfill(7)\n",
    "\n",
    "print(\"date_col:\" + str(date_col))\n",
    "print(\"time_col:\" + str(time_col))\n",
    "\n",
    "print(date_length)\n",
    "print(df[date_col].head())\n",
    "\n",
    "if date_length == 6:\n",
    "    decade = int(df[date_col].at[0][:2])\n",
    "    if decade < 50:\n",
    "        century = str(date.today())[:2]\n",
    "    else:\n",
    "        century = int(str(date.today())[:2])-1\n",
    "    df[date_col] = str(century) + df[date_col]\n",
    "elif date_length == 8:\n",
    "    pass\n",
    "else:\n",
    "    raise Exception(\"DATA Date length abnormal. length = \"+str(date_length))\n",
    "\n",
    "if typ == \"indv\":\n",
    "    df[date_col] = pd.to_datetime(df[date_col], format=\"%Y%m%d\").dt.strftime(date_format)\n",
    "else:\n",
    "    df[date_col] = df[date_col].apply(lambda x: pd.to_datetime(\n",
    "        x, format=\"%Y%m%d\").date() + timedelta(days=add_day)\n",
    "        if x[time_col] in ['0'.zfill(7),'24'.ljust(7,'0')] \n",
    "        else pd.to_datetime(x, format=\"%Y%m%d\").date())\n",
    "\n",
    "df[time_col] = pd.to_datetime(df[time_col], format=\"%H%M%S%f\").dt.strftime(time_format)\n",
    "# df[time_col] = df[time_col].str[:11].astype(str)\n",
    "print((df[date_col].astype(str)+df[time_col].astype(str)).head())\n",
    "\n",
    "\n",
    "df['end_datetime'] = pd.to_datetime((df[date_col].astype(str)+df[time_col].str[:10].astype(str)), \n",
    "    format=date_format+time_format)\n",
    "\n",
    "df['start_datetime'] = pd.to_datetime((df[date_col].astype(str)+df[time_col].str[:10].astype(str)), \n",
    "    format=date_format+time_format) - timedelta(minutes=duration_min)\n",
    "\n",
    "df.rename(columns={\n",
    "    date_col: date_col_name,\n",
    "    time_col: time_col_name\n",
    "},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dfa.empty:\n",
    "    lanes = dfa.loc[\n",
    "        dfa[0]==\"L1\"\n",
    "        ].dropna(axis=1).drop_duplicates().reset_index(drop=True).copy()\n",
    "    lanes = lanes.drop([17, 18, 19, 20, 21, 22, 23, 24, 25], axis=1, errors='ignore')\n",
    "    if lanes.shape[1] == 5:\n",
    "        lanes.rename(columns={\n",
    "            1 : \"lane_number\",\n",
    "            2 : \"direction_code\",\n",
    "            3 : \"lane_type_code\",\n",
    "            4 : \"traffic_stream_number\"\n",
    "        },inplace = True)\n",
    "    elif lanes.shape[1] == 11:\n",
    "        lanes.rename(columns={\n",
    "            1 : \"lane_number\",\n",
    "            2 : \"direction_code\",\n",
    "            3 : \"lane_type_code\",\n",
    "            4 : \"traffic_stream_number\",\n",
    "            5 : \"traffic_stream_lane_position\",\n",
    "            6 : \"reverse_direction_lane_number\",\n",
    "            7 : \"vehicle_code\",\n",
    "            8 : \"time_code\",\n",
    "            9 : \"length_code\",\n",
    "            10 : \"speed_code\"\n",
    "        },inplace = True)\n",
    "    elif lanes.shape[1] == 17:\n",
    "        lanes.rename(columns={\n",
    "            1 : \"lane_number\",\n",
    "            2 : \"direction_code\",\n",
    "            3 : \"lane_type_code\",\n",
    "            4 : \"traffic_stream_number\",\n",
    "            5 : \"traffic_stream_lane_position\",\n",
    "            6 : \"reverse_direction_lane_number\",\n",
    "            7 : \"vehicle_code\",\n",
    "            8 : \"time_code\",\n",
    "            9 : \"length_code\",\n",
    "            10 : \"speed_code\",\n",
    "            11 : \"occupancy_time_code\",\n",
    "            12 : \"vehicle_following_code\",\n",
    "            13 : \"trailer_code\",\n",
    "            14 : \"axle_code\",\n",
    "            15 : \"mass_code\",\n",
    "            16 : \"tyre_type_code\"\n",
    "        },inplace = True)\n",
    "    else:\n",
    "        lanes.rename(columns={\n",
    "            1 : \"lane_number\",\n",
    "            2 : \"direction_code\",\n",
    "            3 : \"lane_type_code\",\n",
    "            4 : \"traffic_stream_number\"\n",
    "        },inplace = True)\n",
    "    lanes[lanes.select_dtypes(include=['object']).columns] = lanes[\n",
    "        lanes.select_dtypes(include=['object']).columns].apply(\n",
    "            pd.to_numeric, axis=1, errors='ignore')\n",
    "    lanes[\"site_name\"] = 'site_id'\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_1 = lanes[\"direction_code\"].astype(int).min()\n",
    "    dir_2 = lanes[\"direction_code\"].astype(int).max()\n",
    "except (TypeError,ValueError):\n",
    "    dir_1 = 0\n",
    "    dir_2 = 4\n",
    "    \n",
    "if df.loc[df[0]==\"10\"].empty:\n",
    "    lane_col = 5\n",
    "else:\n",
    "    lane_col = 6\n",
    "\n",
    "\n",
    "df['direction'] = df[lane_col].astype(int)\n",
    "df['compass_heading'] = df[lane_col].astype(int)\n",
    "df['direction_code'] = df[lane_col].astype(int)\n",
    "df['direction'].loc[df[lane_col].astype(int).isin(\n",
    "    list(lanes['lane_number'].astype(int).loc[lanes['direction_code'].astype(int) == dir_1])\n",
    "    )] = 'P'\n",
    "df['direction'].loc[df[lane_col].astype(int).isin(\n",
    "    list(lanes['lane_number'].astype(int).loc[lanes['direction_code'].astype(int) == dir_2])\n",
    "    )] = 'N'\n",
    "df['compass_heading'].loc[df[lane_col].astype(int).isin(\n",
    "    list(lanes['lane_number'].astype(int).loc[lanes['direction_code'].astype(int) == dir_1])\n",
    "    )] = str(dir_1)\n",
    "df['compass_heading'].loc[df[lane_col].astype(int).isin(\n",
    "    list(lanes['lane_number'].astype(int).loc[lanes['direction_code'].astype(int) == dir_2])\n",
    "    )] = str(dir_2)\n",
    "df['direction_code'].loc[df[lane_col].astype(int).isin(\n",
    "    list(lanes['lane_number'].astype(int).loc[lanes['direction_code'].astype(int) == dir_2])\n",
    "    )] = str(dir_2)\n",
    "df['direction_code'].loc[df[lane_col].astype(int).isin(\n",
    "    list(lanes['lane_number'].astype(int).loc[lanes['direction_code'].astype(int) == dir_1])\n",
    "    )] = str(dir_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[(df[0] == \"21\")].dropna(\n",
    "    axis=1, how=\"all\"\n",
    ").reset_index(drop=True).copy()\n",
    "\n",
    "if (data[1] == \"0\").any():\n",
    "    ddf = data\n",
    "    # ddf = data.iloc[:, 2:]\n",
    "\n",
    "    ddf.rename(columns = {\n",
    "        4 : \"duration_min\",\n",
    "        5 : \"lane_number\",\n",
    "        6 : \"speedbin1\",\n",
    "        7 : \"speedbin2\",\n",
    "        8 : \"speedbin3\",\n",
    "        9 : \"speedbin4\",\n",
    "        10 : \"speedbin5\",\n",
    "        11 : \"speedbin6\",\n",
    "        12 : \"speedbin7\",\n",
    "        13 : \"speedbin8\",\n",
    "        14 : \"speedbin9\",\n",
    "        15 : \"speedbin10\",\n",
    "        16 : \"sum_of_heavy_vehicle_speeds\",\n",
    "        17 : \"short_heavy_vehicles\",\n",
    "        18 : \"medium_heavy_vehicles\",\n",
    "        19 : \"long_heavy_vehicles\",\n",
    "        20 : \"rear_to_rear_headway_shorter_than_2_seconds\",\n",
    "        21 : \"rear_to_rear_headways_shorter_than_programmed_time\"\n",
    "        }, inplace=True)\n",
    "    ddf[\"speedbin0\"] = 0\n",
    "\n",
    "elif (data[1] == \"1\").any():\n",
    "    ddf = data\n",
    "    # ddf = data.iloc[:, 3:]\n",
    "    \n",
    "    ddf.rename(columns = {\n",
    "        4 : \"duration_min\",\n",
    "        5 : \"lane_number\",\n",
    "        6 : \"speedbin0\",\n",
    "        7 : \"speedbin1\",\n",
    "        8 : \"speedbin2\",\n",
    "        9 : \"speedbin3\",\n",
    "        10 : \"speedbin4\",\n",
    "        11 : \"speedbin5\",\n",
    "        12 : \"speedbin6\",\n",
    "        13 : \"speedbin7\",\n",
    "        14 : \"speedbin8\",\n",
    "        15 : \"speedbin9\",\n",
    "        16 : \"speedbin10\",\n",
    "        17 : \"sum_of_heavy_vehicle_speeds\",\n",
    "        18 : \"short_heavy_vehicles\",\n",
    "        19 : \"medium_heavy_vehicles\",\n",
    "        20 : \"long_heavy_vehicles\",\n",
    "        21 : \"rear_to_rear_headway_shorter_than_2_seconds\",\n",
    "        22 : \"rear_to_rear_headways_shorter_than_programmed_time\",\n",
    "    },inplace=True)\n",
    "\n",
    "ddf = ddf.fillna(0)\n",
    "\n",
    "ddf[\"duration_min\"] = ddf[\"duration_min\"].astype(int)\n",
    "ddf[\"lane_number\"] = ddf[\"lane_number\"].astype(int)\n",
    "ddf[\"speedbin0\"] = ddf[\"speedbin0\"].astype(int)\n",
    "ddf[\"speedbin1\"] = ddf[\"speedbin1\"].astype(int)\n",
    "ddf[\"speedbin2\"] = ddf[\"speedbin2\"].astype(int)\n",
    "ddf[\"speedbin3\"] = ddf[\"speedbin3\"].astype(int)\n",
    "ddf[\"speedbin4\"] = ddf[\"speedbin4\"].astype(int)\n",
    "ddf[\"speedbin5\"] = ddf[\"speedbin5\"].astype(int)\n",
    "ddf[\"speedbin6\"] = ddf[\"speedbin6\"].astype(int)\n",
    "ddf[\"speedbin7\"] = ddf[\"speedbin7\"].astype(int)\n",
    "ddf[\"speedbin8\"] = ddf[\"speedbin8\"].astype(int)\n",
    "ddf[\"speedbin9\"] = ddf[\"speedbin9\"].astype(int)\n",
    "ddf[\"speedbin10\"] = ddf[\"speedbin10\"].astype(int)\n",
    "ddf[\"sum_of_heavy_vehicle_speeds\"] = ddf[\n",
    "    \"sum_of_heavy_vehicle_speeds\"\n",
    "].astype(int)\n",
    "ddf[\"short_heavy_vehicles\"] = ddf[\"short_heavy_vehicles\"].astype(int)\n",
    "ddf[\"medium_heavy_vehicles\"] = ddf[\"medium_heavy_vehicles\"].astype(int)\n",
    "ddf[\"long_heavy_vehicles\"] = ddf[\"long_heavy_vehicles\"].astype(int)\n",
    "ddf[\"rear_to_rear_headway_shorter_than_2_seconds\"] = ddf[\n",
    "    \"rear_to_rear_headway_shorter_than_2_seconds\"\n",
    "].astype(int)\n",
    "ddf[\"rear_to_rear_headways_shorter_than_programmed_time\"] = ddf[\n",
    "    \"rear_to_rear_headways_shorter_than_programmed_time\"\n",
    "].astype(int)\n",
    "\n",
    "ddf[\"total_heavy_vehicles_type21\"] = (\n",
    "    ddf[\"short_heavy_vehicles\"]\n",
    "    + ddf[\"medium_heavy_vehicles\"]\n",
    "    + ddf[\"long_heavy_vehicles\"]\n",
    ")\n",
    "ddf[\"total_heavy_vehicles_type21\"] = ddf[\n",
    "    \"total_heavy_vehicles_type21\"\n",
    "].astype(int)\n",
    "\n",
    "ddf[\"total_light_vehicles_type21\"] = (\n",
    "    ddf[\"speedbin1\"]\n",
    "    + ddf[\"speedbin2\"]\n",
    "    + ddf[\"speedbin3\"]\n",
    "    + ddf[\"speedbin4\"]\n",
    "    + ddf[\"speedbin5\"]\n",
    "    + ddf[\"speedbin6\"]\n",
    "    + ddf[\"speedbin7\"]\n",
    "    + ddf[\"speedbin8\"]\n",
    "    + ddf[\"speedbin9\"]\n",
    "    + ddf[\"speedbin10\"]\n",
    "    - ddf[\"short_heavy_vehicles\"]\n",
    "    - ddf[\"medium_heavy_vehicles\"]\n",
    "    - ddf[\"long_heavy_vehicles\"]\n",
    ")\n",
    "ddf[\"total_light_vehicles_type21\"] = ddf[\n",
    "    \"total_light_vehicles_type21\"\n",
    "].astype(int)\n",
    "\n",
    "ddf[\"total_vehicles_type21\"] = (\n",
    "    ddf[\"speedbin1\"]\n",
    "    + ddf[\"speedbin2\"]\n",
    "    + ddf[\"speedbin3\"]\n",
    "    + ddf[\"speedbin4\"]\n",
    "    + ddf[\"speedbin5\"]\n",
    "    + ddf[\"speedbin6\"]\n",
    "    + ddf[\"speedbin7\"]\n",
    "    + ddf[\"speedbin8\"]\n",
    "    + ddf[\"speedbin9\"]\n",
    "    + ddf[\"speedbin10\"]\n",
    ")\n",
    "ddf[\"total_vehicles_type21\"] = ddf[\"total_vehicles_type21\"].astype(int)\n",
    "\n",
    "# ddf[\"start_datetime\"] = pd.to_datetime(str(ddf[\"start_datetime\"]), \n",
    "# format=\"%Y-%m-%d %H:%M:%S\")\n",
    "try:\n",
    "    ddf['year'] = ddf['start_datetime'].dt.year\n",
    "except AttributeError:\n",
    "    ddf['year'] = int(ddf['start_datetime'].str[:4][0])\n",
    "\n",
    "ddf[\"site_id\"] = \"site_id\"\n",
    "\n",
    "# ddf = ddf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_df is None:\n",
    "    pass\n",
    "else:\n",
    "    data = data_df.loc[(data_df[0] == \"60\")].dropna(\n",
    "        axis=1, how=\"all\"\n",
    "    ).reset_index(drop=True).copy()\n",
    "    dfh = head_df.loc[(head_df[0] == \"60\")].dropna(\n",
    "                axis=1, how=\"all\"\n",
    "            ).drop_duplicates().reset_index(drop=True).copy()\n",
    "    dfh['error_bin'] = 0\n",
    "\n",
    "    number_of_data_records = dfh.iloc[0,3]\n",
    "\n",
    "    if data[1].isin([\"0\", \"1\", \"2\", \"3\", \"4\"]).any():\n",
    "        ddf = data.iloc[:, 1:].reset_index(drop=True)\n",
    "        ddf = pd.DataFrame(ddf).dropna(axis=1, how=\"all\").reset_index(drop=True)\n",
    "\n",
    "        ddf[ddf.select_dtypes(include=['object']).columns] = ddf[\n",
    "            ddf.select_dtypes(include=['object']).columns\n",
    "            ].apply(pd.to_numeric, axis = 1, errors='ignore')\n",
    "\n",
    "        max_lanes = lanes['lane_number'].astype(int).max()\n",
    "\n",
    "        ddf.columns = ddf.columns.astype(str)\n",
    "\n",
    "        df3 = pd.DataFrame(columns=[\n",
    "        'edit_code', \n",
    "        'start_datetime', \n",
    "        'end_date', \n",
    "        'end_time', \n",
    "        'duration_of_summary', \n",
    "        'lane_number', \n",
    "        'bin_number', \n",
    "        'number_of_vehicles', \n",
    "        'bin_boundary_length_cm', \n",
    "        'direction', \n",
    "        'compass_heading'])\n",
    "\n",
    "        for i in range(6,int(number_of_data_records)+6):\n",
    "            for lane_no in range(1, max_lanes+1):\n",
    "                join_to_df3 = ddf.loc[ddf['5'].astype(int) == lane_no, [\n",
    "                    '1', # edit_code\n",
    "                    'start_datetime',\n",
    "                    'end_date', # end_date\n",
    "                    'end_time', # end_time\n",
    "                    '4', # duration_of_summary\n",
    "                    '5', # lane_number\n",
    "                    str(i), # \"number_of_vehicles\"\n",
    "                    'direction', \n",
    "                    'compass_heading'\n",
    "                    ]]\n",
    "                if str(int(i)-6) == \"0\":\n",
    "                    bin_bound_col = 'error_bin'\n",
    "                else:\n",
    "                    bin_bound_col = int(i)-3\n",
    "                join_to_df3['bin_number'] = str(i-6)\n",
    "                join_to_df3['bin_boundary_length_cm'] = int(dfh[bin_bound_col][0])\n",
    "                join_to_df3.rename(columns={\n",
    "                    '1':\"edit_code\",\n",
    "                    '4':\"duration_of_summary\",\n",
    "                    '5':\"lane_number\",\n",
    "                    str(i): \"number_of_vehicles\"\n",
    "                    }, inplace=True)\n",
    "                df3 = pd.concat([df3,join_to_df3], axis=0, ignore_index=True)\n",
    "        df3 = df3.apply(pd.to_numeric, axis = 1, errors=\"ignore\")\n",
    "        df3['site_id'] = \"site_id\"\n",
    "        df3['year'] = int(df3['start_datetime'].at[0].year)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['year'] = int(df3['start_datetime'].at[0].year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(lanes['lane_number'].astype(int).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6  :  0  :  error_bin\n",
      "7  :  1  :  4\n",
      "8  :  2  :  5\n",
      "9  :  3  :  6\n",
      "10  :  4  :  7\n",
      "11  :  5  :  8\n",
      "12  :  6  :  9\n",
      "13  :  7  :  10\n",
      "14  :  8  :  11\n",
      "15  :  9  :  12\n",
      "16  :  10  :  13\n",
      "17  :  11  :  14\n",
      "18  :  12  :  15\n",
      "19  :  13  :  16\n",
      "20  :  14  :  17\n",
      "21  :  15  :  18\n",
      "22  :  16  :  19\n",
      "23  :  17  :  20\n",
      "24  :  18  :  21\n",
      "25  :  19  :  22\n"
     ]
    }
   ],
   "source": [
    "for i in range(6,int(number_of_data_records)+6):\n",
    "    if str(int(i)-6) == \"0\":\n",
    "        x = 'error_bin'\n",
    "    else:\n",
    "        x = str(int(i)-3)\n",
    "    print(i, \" : \",str(int(i)-6), \" : \", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "23",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 23",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\test.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000016?line=0'>1</a>\u001b[0m dfh[\u001b[39m23\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 23"
     ]
    }
   ],
   "source": [
    "dfh[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000508</td>\n",
       "      <td>100000</td>\n",
       "      <td>20000508</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000509</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000509</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000510</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000510</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000511</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000511</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000512</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000512</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000513</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000513</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000514</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000514</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000515</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000515</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000516</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000516</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000517</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000517</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000518</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000518</td>\n",
       "      <td>234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>D3</td>\n",
       "      <td>20000519</td>\n",
       "      <td>0-5500</td>\n",
       "      <td>20000519</td>\n",
       "      <td>214500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1       2         3       4\n",
       "3    D3  20000508  100000  20000508  234500\n",
       "15   D3  20000509  0-5500  20000509  234500\n",
       "27   D3  20000510  0-5500  20000510  234500\n",
       "39   D3  20000511  0-5500  20000511  234500\n",
       "51   D3  20000512  0-5500  20000512  234500\n",
       "63   D3  20000513  0-5500  20000513  234500\n",
       "75   D3  20000514  0-5500  20000514  234500\n",
       "87   D3  20000515  0-5500  20000515  234500\n",
       "99   D3  20000516  0-5500  20000516  234500\n",
       "111  D3  20000517  0-5500  20000517  234500\n",
       "123  D3  20000518  0-5500  20000518  234500\n",
       "135  D3  20000519  0-5500  20000519  214500"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[0].isin([\"D1\",\"D3\"]), 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def header(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "head_df = head_df.reset_index(drop=True)\n",
    "st_nd = head_df.loc[head_df[0].isin([\"D1\",\"D3\"]), 0:4].reset_index(drop=True).copy()\n",
    "\n",
    "# adds century to year if it is not there\n",
    "st_nd[1] = st_nd[1].apply(lambda x: str(date.today())[:2] + x if len(x)==6 else x)\n",
    "st_nd[3] = st_nd[3].apply(lambda x: str(date.today())[:2] + x if len(x)==6 else x)\n",
    "\n",
    "st_nd[1] = st_nd[1].str.pad(width=8, side='right', fillchar=\"0\")\n",
    "st_nd[3] = st_nd[3].str.pad(width=8, side='right', fillchar=\"0\")\n",
    "\n",
    "# index 2 and 4 are time, this makes the time uniform length\n",
    "st_nd[2] = st_nd[2].str.pad(width=7,side='right',fillchar=\"0\")\n",
    "st_nd[4] = st_nd[4].str.pad(width=7,side='right',fillchar=\"0\")\n",
    "\n",
    "# this filters for time = 24H00m and makes it zero hour\n",
    "st_nd[2].loc[st_nd[2].str[:2] == '24'] = ('0').zfill(7)\n",
    "st_nd[4].loc[st_nd[4].str[:2] == '24'] = ('0').zfill(7)\n",
    "\n",
    "# adds a day if the hour is zero hour and changes string to dtetime.date\n",
    "try:\n",
    "    st_nd[1] = st_nd[1].apply(lambda x: pd.to_datetime(x, format=\"%Y%m%d\").date() + timedelta(days=1)\n",
    "    if x[2] in ['0'.zfill(7),'24'.zfill(7)] else pd.to_datetime(x, format=\"%Y%m%d\").date())\n",
    "    st_nd[3] = st_nd[3].apply(lambda x: pd.to_datetime(x, format=\"%Y%m%d\").date() + timedelta(days=1)\n",
    "    if x[4] in ['0'.zfill(7),'24'.zfill(7)] else pd.to_datetime(x, format=\"%Y%m%d\").date())\n",
    "# except ValueError:\n",
    "#     st_nd[1] = st_nd[1].apply(lambda x: pd.to_datetime(x[:8], format=\"%Y%m%d\").date() + timedelta(days=1)\n",
    "#     if x[2] in ['0'.zfill(7),'24'.zfill(7)] else pd.to_datetime(x[:8], format=\"%Y%m%d\").date())\n",
    "#     st_nd[3] = st_nd[3].apply(lambda x: pd.to_datetime(x[:8], format=\"%Y%m%d\").date() + timedelta(days=1)\n",
    "#     if x[4] in ['0'.zfill(7),'24'.zfill(7)] else pd.to_datetime(x[:8], format=\"%Y%m%d\").date())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# changes time string into datetime.time\n",
    "try:\n",
    "    st_nd[2] = st_nd[2].apply(lambda x: pd.to_datetime(x, format=\"%H%M%S%f\").time())\n",
    "    st_nd[4] = st_nd[4].apply(lambda x: pd.to_datetime(x, format=\"%H%M%S%f\").time())\n",
    "except ValueError:\n",
    "    st_nd[2] = pd.to_datetime(\"0\".zfill(7), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")\n",
    "    st_nd[4] = pd.to_datetime(\"0\".zfill(7), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# creates start_datetime and end_datetime\n",
    "try:\n",
    "    st_nd[\"start_datetime\"] = pd.to_datetime((st_nd[1].astype(str)+st_nd[2].astype(str)), \n",
    "        format='%Y-%m-%d%H:%M:%S')\n",
    "    st_nd[\"end_datetime\"] = pd.to_datetime((st_nd[3].astype(str)+st_nd[4].astype(str)), \n",
    "        format='%Y-%m-%d%H:%M:%S')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "st_nd = st_nd.iloc[:,1:].drop_duplicates()\n",
    "\n",
    "headers = pd.DataFrame()\n",
    "try:\n",
    "    headers['start_datetime'] = st_nd.groupby(st_nd['start_datetime'].dt.year).min()['start_datetime']\n",
    "    headers['end_datetime'] = st_nd.groupby(st_nd['end_datetime'].dt.year).max()['end_datetime']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "headers['site_id'] = 'site_id'\n",
    "headers['document_url'] = 'document_url'\n",
    "\n",
    "headers[\"header_id\"] = \"\"\n",
    "headers[\"header_id\"] = headers[\"header_id\"].apply(\n",
    "            lambda x: str(uuid.uuid4()))\n",
    "\n",
    "headers['year'] = headers['start_datetime'].dt.year\n",
    "\n",
    "headers[\"number_of_lanes\"] = int(head_df.loc[head_df[0] == \"L0\", 2].drop_duplicates().reset_index(drop=True)[0])\n",
    "\n",
    "station_name = head_df.loc[head_df[0].isin([\"S0\"]), 3:].reset_index(drop=True).drop_duplicates().dropna(axis=1)\n",
    "headers[\"station_name\"] = station_name[station_name.columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "t21 = head_df.loc[head_df[0]==\"21\"].dropna(axis=1).drop_duplicates().reset_index().copy()\n",
    "t21 = t21.iloc[:,(t21.shape[1])-9:].astype(int)\n",
    "t21.columns = range(t21.columns.size)\n",
    "t21.rename(columns={\n",
    "    0:'speedbin1',\n",
    "    1:'speedbin2',\n",
    "    2:'speedbin3',\n",
    "    3:'speedbin4',\n",
    "    4:'speedbin5',\n",
    "    5:'speedbin6',\n",
    "    6:'speedbin7',\n",
    "    7:'speedbin8',\n",
    "    8:'speedbin9'}\n",
    "    ,inplace = True)\n",
    "\n",
    "headers = pd.concat([headers,t21],ignore_index=True,axis=0).bfill().ffill().drop_duplicates()\n",
    "\n",
    "try:\n",
    "    headers[\"type_21_count_interval_minutes\"] = int(head_df.loc[head_df[0]==\"21\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_30_summary_interval_minutes\"] = int(head_df.loc[head_df[0]==\"21\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_70_summary_interval_minutes\"] = int(head_df.loc[head_df[0]==\"21\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "except KeyError:\n",
    "    pass\n",
    "try:\n",
    "    headers[\"type_30_summary_interval_minutes\"] = int(head_df.loc[head_df[0]==\"30\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_21_count_interval_minutes\"] = int(head_df.loc[head_df[0]==\"30\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_70_summary_interval_minutes\"] = int(head_df.loc[head_df[0]==\"30\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_30_vehicle_classification_scheme\"] = int(head_df.loc[head_df[0] == \"30\", 3].drop_duplicates().reset_index(drop=True)[0])\n",
    "except KeyError:\n",
    "    pass\n",
    "try:\n",
    "    headers[\"type_30_summary_interval_minutes\"] = int(head_df.loc[head_df[0]==\"70\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_21_count_interval_minutes\"] = int(head_df.loc[head_df[0]==\"70\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_70_summary_interval_minutes\"] = int(head_df.loc[head_df[0]==\"70\",1].drop_duplicates().reset_index(drop=True)[0])\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    headers['dir1_id'] = int(head_df[head_df[0]==\"L1\"].dropna(axis=1).drop_duplicates().reset_index(drop=True)[2].min())\n",
    "    headers['dir2_id'] = int(head_df[head_df[0]==\"L1\"].dropna(axis=1).drop_duplicates().reset_index(drop=True)[2].max())\n",
    "except (KeyError, ValueError):\n",
    "    headers['dir1_id'] = 0\n",
    "    headers['dir2_id'] = 4\n",
    "\n",
    "try:\n",
    "    headers[\"type_70_vehicle_classification_scheme\"] = int(head_df.loc[\n",
    "        head_df[0] == \"70\", 2].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_70_maximum_gap_milliseconds\"] = int(head_df.loc[\n",
    "        head_df[0] == \"70\", 3].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_70_maximum_differential_speed\"] = int(head_df.loc[\n",
    "        head_df[0] == \"70\", 4].drop_duplicates().reset_index(drop=True)[0])\n",
    "    headers[\"type_70_error_bin_code\"] = int(head_df.loc[\n",
    "        head_df[0] == \"70\", 5].drop_duplicates().reset_index(drop=True)[0])\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "headers = headers.reset_index(drop=True)\n",
    "\n",
    "m = headers.select_dtypes(np.number)\n",
    "headers[m.columns] = m.round().astype('Int32')\n",
    "\n",
    "# return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>site_id</th>\n",
       "      <th>document_url</th>\n",
       "      <th>header_id</th>\n",
       "      <th>year</th>\n",
       "      <th>number_of_lanes</th>\n",
       "      <th>station_name</th>\n",
       "      <th>speedbin1</th>\n",
       "      <th>speedbin2</th>\n",
       "      <th>speedbin3</th>\n",
       "      <th>speedbin4</th>\n",
       "      <th>speedbin5</th>\n",
       "      <th>speedbin6</th>\n",
       "      <th>speedbin7</th>\n",
       "      <th>speedbin8</th>\n",
       "      <th>speedbin9</th>\n",
       "      <th>type_21_count_interval_minutes</th>\n",
       "      <th>type_30_summary_interval_minutes</th>\n",
       "      <th>type_70_summary_interval_minutes</th>\n",
       "      <th>dir1_id</th>\n",
       "      <th>dir2_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-10-05 14:45:00</td>\n",
       "      <td>2011-10-13 11:15:00</td>\n",
       "      <td>site_id</td>\n",
       "      <td>document_url</td>\n",
       "      <td>bb7e011f-773a-4e5d-9821-2e025fbceb4b</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start_datetime        end_datetime  site_id  document_url  \\\n",
       "0 2011-10-05 14:45:00 2011-10-13 11:15:00  site_id  document_url   \n",
       "\n",
       "                              header_id  year  number_of_lanes  station_name  \\\n",
       "0  bb7e011f-773a-4e5d-9821-2e025fbceb4b  2011                2          <NA>   \n",
       "\n",
       "   speedbin1  speedbin2  speedbin3  speedbin4  speedbin5  speedbin6  \\\n",
       "0         30         40         50         60         70         80   \n",
       "\n",
       "   speedbin7  speedbin8  speedbin9  type_21_count_interval_minutes  \\\n",
       "0         90        100        110                              15   \n",
       "\n",
       "   type_30_summary_interval_minutes  type_70_summary_interval_minutes  \\\n",
       "0                                15                                15   \n",
       "\n",
       "   dir1_id  dir2_id  \n",
       "0        0        0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_21(df) -> pd.DataFrame: # deals with type 21\n",
    "    if df is None:\n",
    "        pass\n",
    "    else:\n",
    "        data = df.loc[(df[0] == \"21\")].dropna(\n",
    "            axis=1, how=\"all\"\n",
    "        ).reset_index(drop=True).copy()\n",
    "\n",
    "        if (data[1] == \"0\").any():\n",
    "            ddf = data\n",
    "            # ddf = data.iloc[:, 2:]\n",
    "\n",
    "            ddf.rename(columns = {\n",
    "                4 : \"duration_min\",\n",
    "                5 : \"lane_number\",\n",
    "                6 : \"speedbin1\",\n",
    "                7 : \"speedbin2\",\n",
    "                8 : \"speedbin3\",\n",
    "                9 : \"speedbin4\",\n",
    "                10 : \"speedbin5\",\n",
    "                11 : \"speedbin6\",\n",
    "                12 : \"speedbin7\",\n",
    "                13 : \"speedbin8\",\n",
    "                14 : \"speedbin9\",\n",
    "                15 : \"speedbin10\",\n",
    "                16 : \"sum_of_heavy_vehicle_speeds\",\n",
    "                17 : \"short_heavy_vehicles\",\n",
    "                18 : \"medium_heavy_vehicles\",\n",
    "                19 : \"long_heavy_vehicles\",\n",
    "                20 : \"rear_to_rear_headway_shorter_than_2_seconds\",\n",
    "                21 : \"rear_to_rear_headways_shorter_than_programmed_time\"\n",
    "                }, inplace=True)\n",
    "            ddf[\"speedbin0\"] = 0\n",
    "\n",
    "        elif (data[1] == \"1\").any():\n",
    "            ddf = data\n",
    "            # ddf = data.iloc[:, 3:]\n",
    "            \n",
    "            ddf.rename(columns = {\n",
    "                4 : \"duration_min\",\n",
    "                5 : \"lane_number\",\n",
    "                6 : \"speedbin0\",\n",
    "                7 : \"speedbin1\",\n",
    "                8 : \"speedbin2\",\n",
    "                9 : \"speedbin3\",\n",
    "                10 : \"speedbin4\",\n",
    "                11 : \"speedbin5\",\n",
    "                12 : \"speedbin6\",\n",
    "                13 : \"speedbin7\",\n",
    "                14 : \"speedbin8\",\n",
    "                15 : \"speedbin9\",\n",
    "                16 : \"speedbin10\",\n",
    "                17 : \"sum_of_heavy_vehicle_speeds\",\n",
    "                18 : \"short_heavy_vehicles\",\n",
    "                19 : \"medium_heavy_vehicles\",\n",
    "                20 : \"long_heavy_vehicles\",\n",
    "                21 : \"rear_to_rear_headway_shorter_than_2_seconds\",\n",
    "                22 : \"rear_to_rear_headways_shorter_than_programmed_time\",\n",
    "            },inplace=True)\n",
    "        \n",
    "        ddf = ddf.fillna(0)\n",
    "\n",
    "        ddf[\"duration_min\"] = ddf[\"duration_min\"].astype(int)\n",
    "        ddf[\"lane_number\"] = ddf[\"lane_number\"].astype(int)\n",
    "        ddf[\"speedbin0\"] = ddf[\"speedbin0\"].astype(int)\n",
    "        ddf[\"speedbin1\"] = ddf[\"speedbin1\"].astype(int)\n",
    "        ddf[\"speedbin2\"] = ddf[\"speedbin2\"].astype(int)\n",
    "        ddf[\"speedbin3\"] = ddf[\"speedbin3\"].astype(int)\n",
    "        ddf[\"speedbin4\"] = ddf[\"speedbin4\"].astype(int)\n",
    "        ddf[\"speedbin5\"] = ddf[\"speedbin5\"].astype(int)\n",
    "        ddf[\"speedbin6\"] = ddf[\"speedbin6\"].astype(int)\n",
    "        ddf[\"speedbin7\"] = ddf[\"speedbin7\"].astype(int)\n",
    "        ddf[\"speedbin8\"] = ddf[\"speedbin8\"].astype(int)\n",
    "        ddf[\"speedbin9\"] = ddf[\"speedbin9\"].astype(int)\n",
    "        ddf[\"speedbin10\"] = ddf[\"speedbin10\"].astype(int)\n",
    "        ddf[\"sum_of_heavy_vehicle_speeds\"] = ddf[\n",
    "            \"sum_of_heavy_vehicle_speeds\"\n",
    "        ].astype(int)\n",
    "        ddf[\"short_heavy_vehicles\"] = ddf[\"short_heavy_vehicles\"].astype(int)\n",
    "        ddf[\"medium_heavy_vehicles\"] = ddf[\"medium_heavy_vehicles\"].astype(int)\n",
    "        ddf[\"long_heavy_vehicles\"] = ddf[\"long_heavy_vehicles\"].astype(int)\n",
    "        ddf[\"rear_to_rear_headway_shorter_than_2_seconds\"] = ddf[\n",
    "            \"rear_to_rear_headway_shorter_than_2_seconds\"\n",
    "        ].astype(int)\n",
    "        ddf[\"rear_to_rear_headways_shorter_than_programmed_time\"] = ddf[\n",
    "            \"rear_to_rear_headways_shorter_than_programmed_time\"\n",
    "        ].astype(int)\n",
    "\n",
    "        ddf[\"total_heavy_vehicles_type21\"] = (\n",
    "            ddf[\"short_heavy_vehicles\"]\n",
    "            + ddf[\"medium_heavy_vehicles\"]\n",
    "            + ddf[\"long_heavy_vehicles\"]\n",
    "        )\n",
    "        ddf[\"total_heavy_vehicles_type21\"] = ddf[\n",
    "            \"total_heavy_vehicles_type21\"\n",
    "        ].astype(int)\n",
    "\n",
    "        ddf[\"total_light_vehicles_type21\"] = (\n",
    "            ddf[\"speedbin1\"]\n",
    "            + ddf[\"speedbin2\"]\n",
    "            + ddf[\"speedbin3\"]\n",
    "            + ddf[\"speedbin4\"]\n",
    "            + ddf[\"speedbin5\"]\n",
    "            + ddf[\"speedbin6\"]\n",
    "            + ddf[\"speedbin7\"]\n",
    "            + ddf[\"speedbin8\"]\n",
    "            + ddf[\"speedbin9\"]\n",
    "            + ddf[\"speedbin10\"]\n",
    "            - ddf[\"short_heavy_vehicles\"]\n",
    "            - ddf[\"medium_heavy_vehicles\"]\n",
    "            - ddf[\"long_heavy_vehicles\"]\n",
    "        )\n",
    "        ddf[\"total_light_vehicles_type21\"] = ddf[\n",
    "            \"total_light_vehicles_type21\"\n",
    "        ].astype(int)\n",
    "\n",
    "        ddf[\"total_vehicles_type21\"] = (\n",
    "            ddf[\"speedbin1\"]\n",
    "            + ddf[\"speedbin2\"]\n",
    "            + ddf[\"speedbin3\"]\n",
    "            + ddf[\"speedbin4\"]\n",
    "            + ddf[\"speedbin5\"]\n",
    "            + ddf[\"speedbin6\"]\n",
    "            + ddf[\"speedbin7\"]\n",
    "            + ddf[\"speedbin8\"]\n",
    "            + ddf[\"speedbin9\"]\n",
    "            + ddf[\"speedbin10\"]\n",
    "        )\n",
    "        ddf[\"total_vehicles_type21\"] = ddf[\"total_vehicles_type21\"].astype(int)\n",
    "\n",
    "        try:\n",
    "            ddf['year'] = ddf['start_datetime'].dt.year\n",
    "        except AttributeError:\n",
    "            ddf['year'] = int(ddf['start_datetime'].str[:4][0])\n",
    "\n",
    "        ddf[\"site_id\"] = \"site_id\"\n",
    "\n",
    "        return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t21_data = type_21(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_calcs(header: pd.DataFrame, data: pd.DataFrame, type: int) -> pd.DataFrame:\n",
    "    try:\n",
    "        speed_limit_qry = f\"select max_speed from trafc.countstation where tcname = '{data['site_id'].iloc[0]}' ;\"\n",
    "        speed_limit = pd.read_sql_query(speed_limit_qry,config.ENGINE).reset_index(drop=True)\n",
    "        try:\n",
    "            speed_limit = speed_limit['max_speed'].iloc[0]\n",
    "        except IndexError:\n",
    "            speed_limit = 60\n",
    "        data = data.fillna(0, axis=0)\n",
    "        if type == 21:\n",
    "            try:\n",
    "                header['adt_total'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D')]).sum().mean().round().astype(int)\n",
    "                header['adt_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'P']]).sum().mean())\n",
    "                header['adt_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N']]).sum().mean())\n",
    "\n",
    "                header['adtt_total'] = data['total_heavy_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D')]).sum().mean().round().astype(int)\n",
    "                header['adtt_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'P']]).sum().mean())\n",
    "                header['adtt_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N']]).sum().mean())\n",
    "\n",
    "                header['total_vehicles'] = data['total_vehicles_type21'].sum()\n",
    "                header['total_vehicles_positive_direction'] = data['total_vehicles_type21'].groupby(data['direction'].loc[data['direction'] == 'P']).sum()[0]\n",
    "                header['total_vehicles_negative_direction'] = data['total_vehicles_type21'].groupby(data['direction'].loc[data['direction'] == 'N']).sum()[0]\n",
    "\n",
    "                header['total_heavy_vehicles'] = data['total_heavy_vehicles_type21'].sum()\n",
    "                header['total_heavy_negative_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "                header['total_heavy_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                header['truck_split_negative_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0] / data['total_heavy_vehicles_type21'].sum()\n",
    "                header['truck_split_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0] / data['total_heavy_vehicles_type21'].sum()\n",
    "\n",
    "                header['total_light_vehicles'] = data['total_light_vehicles_type21'].sum()\n",
    "                header['total_light_positive_direction'] = data['total_light_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                header['total_light_negative_direction'] = data['total_light_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "\n",
    "                header['short_heavy_vehicles'] = data['short_heavy_vehicles'].sum()\n",
    "                header['short_heavy_positive_direction'] = data['short_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                header['short_heavy_negative_direction'] = data['short_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "\n",
    "                header['Medium_heavy_vehicles'] = data['medium_heavy_vehicles'].sum()\n",
    "                header['Medium_heavy_negative_direction'] = data['medium_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "                header['Medium_heavy_positive_direction'] = data['medium_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "\n",
    "                header['long_heavy_vehicles'] = data['long_heavy_vehicles'].sum()\n",
    "                header['long_heavy_positive_direction'] = data['long_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                header['long_heavy_negative_direction'] = data['long_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "\n",
    "                header['vehicles_with_rear_to_rear_headway_less_than_2sec_positive_dire'] = data['rear_to_rear_headway_shorter_than_2_seconds'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                header['vehicles_with_rear_to_rear_headway_less_than_2sec_negative_dire'] = data['rear_to_rear_headway_shorter_than_2_seconds'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "                header['vehicles_with_rear_to_rear_headway_less_than_2sec_total'] = data['rear_to_rear_headway_shorter_than_2_seconds'].sum()\n",
    "            \n",
    "                header['type_21_count_interval_minutes'] = data['duration_min'].mean()\n",
    "\n",
    "                header['highest_volume_per_hour_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'P']]).sum().max())\n",
    "                header['highest_volume_per_hour_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N']]).sum().max())\n",
    "                header['highest_volume_per_hour_total'] = data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H')]).sum().max()\n",
    "\n",
    "                header['15th_highest_volume_per_hour_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'P']]).sum().quantile(q=0.15,  interpolation='linear'))\n",
    "                header['15th_highest_volume_per_hour_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N']]).sum().quantile(q=0.15,  interpolation='linear'))\n",
    "                header['15th_highest_volume_per_hour_total'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H')]).sum().quantile(q=0.15, interpolation='linear'))\n",
    "                \n",
    "                header['30th_highest_volume_per_hour_positive_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'P']]).sum().quantile(q=0.3,  interpolation='linear'))\n",
    "                header['30th_highest_volume_per_hour_negative_direction'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H'), data['direction'].loc[data['direction'] == 'N']]).sum().quantile(q=0.3, interpolation='linear'))\n",
    "                header['30th_highest_volume_per_hour_total'] = round(data['total_vehicles_type21'].groupby([data['start_datetime'].dt.to_period('H')]).sum().quantile(q=0.3, interpolation='linear'))\n",
    "\n",
    "                # header['average_speed_positive_direction'] = \n",
    "                # header['average_speed_negative_direction'] = \n",
    "                try:\n",
    "                    header['average_speed'] = (int((\n",
    "                    (header['speedbin1'] * data['speedbin1'].sum()) +\n",
    "                    (header['speedbin2'] * data['speedbin2'].sum()) +\n",
    "                    (header['speedbin3'] * data['speedbin3'].sum()) +\n",
    "                    (header['speedbin4'] * data['speedbin4'].sum()) +\n",
    "                    (header['speedbin5'] * data['speedbin5'].sum()) +\n",
    "                    (header['speedbin6'] * data['speedbin6'].sum()) +\n",
    "                    (header['speedbin7'] * data['speedbin7'].sum()) +\n",
    "                    (header['speedbin8'] * data['speedbin8'].sum()) +\n",
    "                    (header['speedbin9'] * data['speedbin9'].sum()) \n",
    "                    ))\n",
    "                    / data['sum_of_heavy_vehicle_speeds'].astype(int).sum()\n",
    "                    )\n",
    "                except TypeError:\n",
    "                    header['average_speed'] = (((\n",
    "                    (header['speedbin1'] * data['speedbin1'].astype(int).sum()) +\n",
    "                    (header['speedbin2'] * data['speedbin2'].astype(int).sum()) +\n",
    "                    (header['speedbin3'] * data['speedbin3'].astype(int).sum()) +\n",
    "                    (header['speedbin4'] * data['speedbin4'].astype(int).sum()) +\n",
    "                    (header['speedbin5'] * data['speedbin5'].astype(int).sum()) +\n",
    "                    (header['speedbin6'] * data['speedbin6'].astype(int).sum()) +\n",
    "                    (header['speedbin7'] * data['speedbin7'].astype(int).sum()) +\n",
    "                    (header['speedbin8'] * data['speedbin8'].astype(int).sum()) +\n",
    "                    (header['speedbin9'] * data['speedbin9'].astype(int).sum()) \n",
    "                    ))\n",
    "                    / data['sum_of_heavy_vehicle_speeds'].astype(int).sum()\n",
    "                    )\n",
    "                # header['average_speed_light_vehicles_positive_direction'] = \n",
    "                # header['average_speed_light_vehicles_negative_direction'] = \n",
    "                header['average_speed_light_vehicles'] = ((\n",
    "                    (header['speedbin1'] * data['speedbin1'].sum()) +\n",
    "                    (header['speedbin2'] * data['speedbin2'].sum()) +\n",
    "                    (header['speedbin3'] * data['speedbin3'].sum()) +\n",
    "                    (header['speedbin4'] * data['speedbin4'].sum()) +\n",
    "                    (header['speedbin5'] * data['speedbin5'].sum()) +\n",
    "                    (header['speedbin6'] * data['speedbin6'].sum()) +\n",
    "                    (header['speedbin7'] * data['speedbin7'].sum()) +\n",
    "                    (header['speedbin8'] * data['speedbin8'].sum()) +\n",
    "                    (header['speedbin9'] * data['speedbin9'].sum()) -\n",
    "                    data['sum_of_heavy_vehicle_speeds'].sum()\n",
    "                    )\n",
    "                    / data['sum_of_heavy_vehicle_speeds'].sum()\n",
    "                    )\n",
    "                \n",
    "                # header['average_speed_heavy_vehicles_positive_direction'] = \n",
    "                # header['average_speed_heavy_vehicles_negative_direction'] = \n",
    "                # header['average_speed_heavy_vehicles'] = \n",
    "                \n",
    "                try:\n",
    "                    header['truck_split_positive_direction'] = (str(\n",
    "                        round(data['short_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0] / \n",
    "                        data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]*100)) \n",
    "                    + ' : ' +\n",
    "                    str(\n",
    "                        round(data['medium_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0] / \n",
    "                        data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]*100)) \n",
    "                    + ' : ' +\n",
    "                    str(\n",
    "                        round(data['long_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0] / \n",
    "                        data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]*100))\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    header['truck_split_negative_direction'] = (str(\n",
    "                        round(data['short_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0] / \n",
    "                        data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()*100)) \n",
    "                        + ' : ' +\n",
    "                    str(\n",
    "                        round(data['medium_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0] / \n",
    "                        data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]*100)) \n",
    "                    + ' : ' +\n",
    "                    str(\n",
    "                        round(data['long_heavy_vehicles'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0] / \n",
    "                        data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]*100))\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                try:\n",
    "                    header['truck_split_total'] = (str(\n",
    "                        round(data['short_heavy_vehicles'].sum() / \n",
    "                        data['total_heavy_vehicles_type21'].sum()*100)) \n",
    "                        + ' : ' +\n",
    "                    str(\n",
    "                        round(data['medium_heavy_vehicles'].sum() / \n",
    "                        data['total_heavy_vehicles_type21'].sum()*100)) \n",
    "                    + ' : ' +\n",
    "                    str(\n",
    "                        round(data['long_heavy_vehicles'].sum() / \n",
    "                        data['total_heavy_vehicles_type21'].sum()*100))\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            except KeyError:\n",
    "                pass\n",
    "            try:\n",
    "                header[\"type_21_count_interval_minutes\"] = header[\"type_21_count_interval_minutes\"].round().astype(int)\n",
    "            except (KeyError, pd.errors.IntCastingNaNError):\n",
    "                pass\n",
    "\n",
    "            return header\n",
    "        \n",
    "        elif type == 30:\n",
    "            try:\n",
    "                if header['adt_total'].isnull().all():\n",
    "                    header['adt_total'] = data['total_vehicles_type30'].groupby([data['start_datetime'].dt.to_period('D')]).sum().mean().astype(int)\n",
    "                    header['adt_positive_direction'] = round(data['total_vehicles_type30'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'P']]).sum().mean())\n",
    "                    header['adt_negative_direction'] = round(data['total_vehicles_type30'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N']]).sum().mean())\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if header['adtt_total'].isnull().all():\n",
    "                    header['adtt_total'] = data['total_heavy_vehicles_type30'].groupby([data['start_datetime'].dt.to_period('D')]).sum().mean().astype(int)\n",
    "                    header['adtt_positive_direction'] = round(data['total_vehicles_type30'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'P']]).sum().mean())\n",
    "                    header['adtt_negative_direction'] = round(data['total_vehicles_type30'].groupby([data['start_datetime'].dt.to_period('D'), data['direction'].loc[data['direction'] == 'N']]).sum().mean())\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if header['total_vehicles'].isnull().all():\n",
    "                    header['total_vehicles'] = data['total_vehicles_type30'].sum()\n",
    "                    header['total_vehicles_positive_direction'] = data['total_vehicles_type30'].groupby(data['direction'].loc[data['direction'] == 'P']).sum()[0]\n",
    "                    header['total_vehicles_negative_direction'] = data['total_vehicles_type30'].groupby(data['direction'].loc[data['direction'] == 'N']).sum()[0]\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                if header['total_heavy_vehicles'].isnull().all():\n",
    "                    header['total_heavy_vehicles'] = data['total_heavy_vehicles_type21'].sum()\n",
    "                    header['total_heavy_negative_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "                    header['total_heavy_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                    header['truck_split_negative_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0] / data['total_heavy_vehicles_type21'].sum()\n",
    "                    header['truck_split_positive_direction'] = data['total_heavy_vehicles_type21'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0] / data['total_heavy_vehicles_type21'].sum()\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if header['total_light_vehicles'].isnull().all():\n",
    "                    header['total_light_vehicles'] = data['total_light_vehicles_type30'].sum()\n",
    "                    header['total_light_positive_direction'] = data['total_light_vehicles_type30'].groupby([data['direction'].loc[data['direction'] == 'P']]).sum()[0]\n",
    "                    header['total_light_negative_direction'] = data['total_light_vehicles_type30'].groupby([data['direction'].loc[data['direction'] == 'N']]).sum()[0]\n",
    "                else:\n",
    "                    pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                header['type_30_vehicle_classification_scheme'] = header['type_30_vehicle_classification_scheme'].round().astype(int)\n",
    "            except (KeyError, pd.errors.IntCastingNaNError):\n",
    "                pass\n",
    "\n",
    "            return header\n",
    "\n",
    "        elif type == 70:\n",
    "\n",
    "            try:\n",
    "                header['type_70_maximum_gap_milliseconds'] = header['type_70_maximum_gap_milliseconds'].round().astype(int)\n",
    "            except (KeyError, pd.errors.IntCastingNaNError):\n",
    "                pass\n",
    "            \n",
    "            return header\n",
    "        \n",
    "        elif type == 10:\n",
    "            header['total_light_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='N')].count()[0].round().astype(int)\n",
    "            header['total_light_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='P')].count()[0].round().astype(int)\n",
    "            header['total_light_vehicles'] = data.loc[data['vehicle_class_code_secondary_scheme']<=1].count()[0].round().astype(int)\n",
    "            header['total_heavy_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count()[0].round().astype(int)\n",
    "            header['total_heavy_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count()[0].round().astype(int)\n",
    "            header['total_heavy_vehicles'] = data.loc[data['vehicle_class_code_secondary_scheme']>1].count()[0].round().astype(int)\n",
    "            header['total_short_heavy_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0].round().astype(int)\n",
    "            header['total_short_heavy_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0].round().astype(int)\n",
    "            header['total_short_heavy_vehicles'] = data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0].round().astype(int)\n",
    "            header['total_medium_heavy_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0].round().astype(int)\n",
    "            header['total_medium_heavy_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0].round().astype(int)\n",
    "            header['total_medium_heavy_vehicles'] = data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0].round().astype(int)\n",
    "            header['total_long_heavy_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0].round().astype(int)\n",
    "            header['total_long_heavy_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0].round().astype(int)\n",
    "            header['total_long_heavy_vehicles'] = data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0].round().astype(int)\n",
    "            header['total_vehicles_negative_direction'] = data.loc[data['direction']=='N'].count()[0].round().astype(int)\n",
    "            header['total_vehicles_positive_direction'] = data.loc[data['direction']=='P'].count()[0].round().astype(int)\n",
    "            header['total_vehicles'] = data.count()[0].round().astype(int)\n",
    "            header['average_speed_negative_direction'] = data.loc[data['direction']=='N']['vehicle_speed'].mean().round(2)\n",
    "            header['average_speed_positive_direction'] = data.loc[data['direction']=='P']['vehicle_speed'].mean().round(2)\n",
    "            header['average_speed'] = data['vehicle_speed'].mean().round(2)\n",
    "            header['average_speed_light_vehicles_negative_direction'] = data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='N')].mean().round(2)\n",
    "            header['average_speed_light_vehicles_positive_direction'] = data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']<=1)&(data['direction']=='P')].mean().round(2)\n",
    "            header['average_speed_light_vehicles'] = data['vehicle_speed'].loc[data['vehicle_class_code_secondary_scheme']<=1].mean().round(2)\n",
    "            header['average_speed_heavy_vehicles_negative_direction'] = data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].mean().round(2)\n",
    "            header['average_speed_heavy_vehicles_positive_direction'] = data['vehicle_speed'].loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].mean().round(2)\n",
    "            header['average_speed_heavy_vehicles'] = data['vehicle_speed'].loc[data['vehicle_class_code_secondary_scheme']>1].mean().round(2)\n",
    "            header['truck_split_negative_direction'] = {str((((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].count())[0])*100).round().astype(int))}\n",
    "            header['truck_split_positive_direction'] = {str((((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()/data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].count())[0])*100).round().astype(int))}\n",
    "            header['truck_split_total'] = {str((((data.loc[data['vehicle_class_code_secondary_scheme']==2].count()/data.loc[data['vehicle_class_code_secondary_scheme']>1].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[data['vehicle_class_code_secondary_scheme']==3].count()/data.loc[data['vehicle_class_code_secondary_scheme']>1].count())[0])*100).round().astype(int)) +\":\"+ str((((data.loc[data['vehicle_class_code_secondary_scheme']==4].count()/data.loc[data['vehicle_class_code_secondary_scheme']>1].count())[0])*100).round().astype(int))}\n",
    "            header['estimated_axles_per_truck_negative_direction'] = ((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0]*2+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0]*5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0]*7)/(data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0])).round(2)\n",
    "            header['estimated_axles_per_truck_positive_direction'] = ((data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0]*2+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0]*5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0]*7)/(data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0]+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0])).round(2)\n",
    "            header['estimated_axles_per_truck_total'] = ((data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0]*2+data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0]*5+data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0]*7)/(data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0]+data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0]+data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0])).round(2)\n",
    "            header['percentage_speeding_positive_direction'] = ((data.loc[(data['vehicle_speed']>speed_limit)&(data['direction']=='P')].count()[0]/data.loc[data['direction'=='P']].count()[0])*100).round(2)\n",
    "            header['percentage_speeding_negative_direction'] = ((data.loc[(data['vehicle_speed']>speed_limit)&(data['direction']=='N')].count()[0]/data.loc[data['direction'=='N']].count()[0])*100).round(2)\n",
    "            header['percentage_speeding_total'] = ((data.loc[data['vehicle_speed']>speed_limit].count()[0]/data.count()[0])*100).round(2)\n",
    "            header['vehicles_with_rear_to_rear_headway_less_than_2sec_negative_dire'] = data.loc[(data['vehicle_following_code']==2)&data['direction']=='N'].count()[0]\n",
    "            header['vehicles_with_rear_to_rear_headway_less_than_2sec_positive_dire'] = data.loc[(data['vehicle_following_code']==2)&data['direction']=='P'].count()[0]\n",
    "            header['vehicles_with_rear_to_rear_headway_less_than_2sec_total'] = data.loc[data['vehicle_following_code']==2].count()[0]\n",
    "            header['estimated_e80_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='N')].count()[0]*0.6+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='N')].count()[0]*2.5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='N')].count()[0]*2.1\n",
    "            header['estimated_e80_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']==2)&(data['direction']=='P')].count()[0]*0.6+data.loc[(data['vehicle_class_code_secondary_scheme']==3)&(data['direction']=='P')].count()[0]*2.5+data.loc[(data['vehicle_class_code_secondary_scheme']==4)&(data['direction']=='P')].count()[0]*2.1\n",
    "            header['estimated_e80_on_road'] = data.loc[data['vehicle_class_code_secondary_scheme']==2].count()[0]*0.6+data.loc[data['vehicle_class_code_secondary_scheme']==3].count()[0]*2.5+data.loc[data['vehicle_class_code_secondary_scheme']==4].count()[0]*2.1\n",
    "            header['adt_negative_direction'] = data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)\n",
    "            header['adt_positive_direction'] = data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)\n",
    "            header['adt_total'] = data.groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)\n",
    "            header['adtt_negative_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='N')].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)\n",
    "            header['adtt_positive_direction'] = data.loc[(data['vehicle_class_code_secondary_scheme']>1)&(data['direction']=='P')].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)\n",
    "            header['adtt_total'] = data.loc[data['vehicle_class_code_secondary_scheme']>1].groupby(pd.Grouper(key='start_datetime',freq='D')).count().mean()[0].round().astype(int)\n",
    "            header['highest_volume_per_hour_negative_direction'] = data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='H')).count().max()[0]\n",
    "            header['highest_volume_per_hour_positive_direction'] = data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='H')).count().max()[0]\n",
    "            header['highest_volume_per_hour_total'] = data.groupby(pd.Grouper(key='start_datetime',freq='H')).count().max()[0]\n",
    "            header[\"15th_highest_volume_per_hour_negative_direction\"] = round(data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.15)[0].round().astype(int))\n",
    "            header[\"15th_highest_volume_per_hour_positive_direction\"] = round(data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.15)[0].round().astype(int))\n",
    "            header[\"15th_highest_volume_per_hour_total\"] = data.groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.15)[0].round().astype(int)\n",
    "            header[\"30th_highest_volume_per_hour_negative_direction\"] = round(data.loc[data['direction']=='N'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.30)[0].round().astype(int))\n",
    "            header[\"30th_highest_volume_per_hour_positive_direction\"] = round(data.loc[data['direction']=='P'].groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.30)[0].round().astype(int))\n",
    "            header[\"30th_highest_volume_per_hour_total\"] = data.groupby(pd.Grouper(key='start_datetime',freq='D')).count().quantile(0.30)[0].round().astype(int)\n",
    "            header[\"15th_percentile_speed_negative_direction\"] = data.loc[data['direction']=='N']['vehicle_speed'].quantile(0.15).round(2)\n",
    "            header[\"15th_percentile_speed_positive_direction\"] = data.loc[data['direction']=='P']['vehicle_speed'].quantile(0.15).round(2)\n",
    "            header[\"15th_percentile_speed_total\"] = data['vehicle_speed'].quantile(0.15).round(2)\n",
    "            header[\"85th_percentile_speed_negative_direction\"] = data.loc[data['direction']=='N']['vehicle_speed'].quantile(0.85).round(2)\n",
    "            header[\"85th_percentile_speed_positive_direction\"] = data.loc[data['direction']=='P']['vehicle_speed'].quantile(0.85).round(2)\n",
    "            header[\"85th_percentile_speed_total\"] = data['vehicle_speed'].quantile(0.85).round(2)\n",
    "            header['avg_weekday_traffic'] = data.groupby(pd.Grouper(key='start_datetime',freq='B')).count().mean()[0].round().astype(int)\n",
    "            header['number_of_days_counted'] = data.groupby([data['start_datetime'].dt.to_period('D')]).count().count()[0]\n",
    "            header['duration_hours'] = data.groupby([data['start_datetime'].dt.to_period('H')]).count().count()[0]\n",
    "\n",
    "            return header\n",
    "\n",
    "        elif type == 60:\n",
    "            \n",
    "            return header\n",
    "        else:\n",
    "            return header\n",
    "    except IndexError:\n",
    "        return header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_datetime\n",
       "2011-10-04       5\n",
       "2011-10-05    1903\n",
       "2011-10-06    5435\n",
       "2011-10-07    6079\n",
       "2011-10-08    3708\n",
       "2011-10-09    2463\n",
       "2011-10-10    5433\n",
       "2011-10-11    5869\n",
       "2011-10-12    5750\n",
       "2011-10-13    2300\n",
       "Freq: D, Name: total_vehicles_type21, dtype: int32"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t21_data['total_vehicles_type21'].groupby([t21_data['start_datetime'].dt.to_period('D')]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\test.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000028?line=0'>1</a>\u001b[0m t21_data[\u001b[39m'\u001b[39;49m\u001b[39mtotal_vehicles_type21\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mgroupby(t21_data[\u001b[39m'\u001b[39;49m\u001b[39mdirection\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mloc[t21_data[\u001b[39m'\u001b[39;49m\u001b[39mdirection\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mP\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49msum()[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\MB2705851\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:955\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    952\u001b[0m     key \u001b[39m=\u001b[39m unpack_1tuple(key)\n\u001b[0;32m    954\u001b[0m \u001b[39mif\u001b[39;00m is_integer(key) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m--> 955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m    958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "t21_data['total_vehicles_type21'].groupby(t21_data['direction'].loc[t21_data['direction'] == 'P']).sum()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\1_Coding\\GitHub\\brandtosaurus\\traffic_electronic_count_ETL\\test.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MB2705851/OneDrive%20-%20Surbana%20Jurong%20Private%20Limited/1_Coding/GitHub/brandtosaurus/traffic_electronic_count_ETL/test.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39mround\u001b[39;49m(t21_data[\u001b[39m'\u001b[39;49m\u001b[39mtotal_vehicles_type21\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mgroupby([t21_data[\u001b[39m'\u001b[39;49m\u001b[39mstart_datetime\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mdt\u001b[39m.\u001b[39;49mto_period(\u001b[39m'\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m'\u001b[39;49m), t21_data[\u001b[39m'\u001b[39;49m\u001b[39mdirection\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mloc[t21_data[\u001b[39m'\u001b[39;49m\u001b[39mdirection\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mP\u001b[39;49m\u001b[39m'\u001b[39;49m]])\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mmean()) \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "round(t21_data['total_vehicles_type21'].groupby([t21_data['start_datetime'].dt.to_period('D'), t21_data['direction'].loc[t21_data['direction'] == 'P']]).sum().mean()) or 0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f0240ba5a37eeffe7d380595dcf16248afe1beab2227011f0d1b1f050a1a57d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
